{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69ad78ae",
   "metadata": {},
   "source": [
    "# Sprint 10 Report\n",
    "\n",
    "---\n",
    "\n",
    "# 3D Skeleton Visualization System\n",
    "\n",
    "## Overview\n",
    "\n",
    "This section details the implementation of a 3D skeleton visualization system for comparing ground truth and predicted skeletal poses. \n",
    "\n",
    "## System Architecture\n",
    "\n",
    "The system is composed of several React components working together:\n",
    "\n",
    "1. **CSVPrediction** - The main container component that handles file uploading, data processing, and state management\n",
    "2. **PoseNet3DVisualization** - Orchestrates the visualization components and manages display modes\n",
    "3. **SkeletonRenderer** - Core Three.js component that renders 3D skeletons with rotation capabilities\n",
    "4. **SkeletonContext & Provider** - Context API implementation for sharing state across components\n",
    "5. **SkeletonControls** - UI controls for animation playback and view options\n",
    "6. **AnimationManager** - Handles animation timing and frame synchronization\n",
    "\n",
    "## Detailed Component Breakdown\n",
    "\n",
    "### 1. CSVPrediction Component\n",
    "\n",
    "This is the main entry point component that handles:\n",
    "\n",
    "- **File Management**: CSV uploads via drag-and-drop or file selection\n",
    "- **Backend Integration**: Loading sample ground truth data from the server\n",
    "- **Data Processing**: Parsing CSV files with automatic delimiter detection\n",
    "- **State Management**: Tracking data state, UI state, and visualization modes\n",
    "- **UI Rendering**: Tab-based interface for data and visualization views\n",
    "\n",
    "#### Key Features:\n",
    "\n",
    "- Adaptive delimiter detection (comma or tab-separated values)\n",
    "- Progress indication during file processing\n",
    "- Data preview with Z-value highlighting\n",
    "- Tab-based interface switching between data view and 3D visualization\n",
    "- Toggle for switching between side-by-side and overlapping visualization modes\n",
    "\n",
    "```javascript\n",
    "// Example of CSV data processing function\n",
    "const processCSV = async (file: File) => {\n",
    "  // Read file content\n",
    "  const text = await file.text();\n",
    "  const lines = text.split(\"\\n\");\n",
    "  \n",
    "  // Detect delimiter (comma or tab)\n",
    "  const firstLine = lines[0];\n",
    "  const delimiter = firstLine.includes(\"\\t\") ? \"\\t\" : \",\";\n",
    "  \n",
    "  // Parse headers and rows...\n",
    "  // Convert to format for 3D visualization...\n",
    "  \n",
    "  // Set as predicted data\n",
    "  setPredictedData(objectRows);\n",
    "};\n",
    "```\n",
    "\n",
    "### 2. PoseNet3DVisualization Component\n",
    "\n",
    "This component orchestrates the visualization experience:\n",
    "\n",
    "- Wraps child components in the SkeletonProvider context\n",
    "- Determines visualization mode (side-by-side or overlapping)\n",
    "- Manages synchronized rendering of multiple SkeletonRenderer instances\n",
    "- Provides feedback for invalid skeletal data\n",
    "\n",
    "```javascript\n",
    "// Rendering logic for different modes\n",
    "{showSideBySide ? (\n",
    "  // Side-by-side view\n",
    "  <div className=\"grid grid-cols-2 gap-4\">\n",
    "    <SkeletonRenderer\n",
    "      poseData={poseData}\n",
    "      isGroundTruth={true}\n",
    "      label={groundTruthLabel}\n",
    "    />\n",
    "    <SkeletonRenderer\n",
    "      poseData={predictedData}\n",
    "      isGroundTruth={false}\n",
    "      label={predictedLabel}\n",
    "    />\n",
    "  </div>\n",
    ") : (\n",
    "  // Overlapping view\n",
    "  <SkeletonRenderer\n",
    "    poseData={poseData}\n",
    "    isGroundTruth={true}\n",
    "    label={`${groundTruthLabel} vs ${predictedLabel}`}\n",
    "    comparisonPoseData={predictedData}\n",
    "  />\n",
    ")}\n",
    "```\n",
    "\n",
    "### 3. SkeletonRenderer Component\n",
    "\n",
    "This is the core visualization component built with Three.js:\n",
    "\n",
    "- **3D Scene Setup**: Canvas creation, camera positioning, lighting\n",
    "- **Skeleton Rendering**: Converting joint data into 3D geometry\n",
    "- **Animation Support**: Handling frame updates with position preservation\n",
    "- **Interaction**: Camera controls for panning, rotation, and zooming\n",
    "- **Visual Feedback**: Color coding and labels for different data types\n",
    "\n",
    "The component follows this initialization sequence:\n",
    "1. Create THREE.js scene, camera, renderer, and lights\n",
    "2. Set up OrbitControls for user interaction\n",
    "3. Add visual elements like labels and legends\n",
    "4. Create the animation loop\n",
    "5. Set up event listeners and cleanups\n",
    "\n",
    "```javascript\n",
    "// Core part of the rendering logic\n",
    "const updateSkeleton = (frameIndex) => {\n",
    "  // Get frame data and create skeleton group\n",
    "  const frameData = poseData[validFrameIndex];\n",
    "  const skeletonGroup = new THREE.Group();\n",
    "  \n",
    "  // Extract joints from frame data\n",
    "  for (const key in frameData) {\n",
    "    if (key.endsWith(\"_x\") || key.endsWith(\"_y\") || key.endsWith(\"_z\")) {\n",
    "      // Extract joint coordinates...\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  // Create joint spheres\n",
    "  Object.keys(joints).forEach((jointName) => {\n",
    "    // Create sphere for each joint...\n",
    "    skeletonGroup.add(sphere);\n",
    "  });\n",
    "  \n",
    "  // Create bone connections\n",
    "  POSE_CONNECTIONS.forEach((connection) => {\n",
    "    // Create cylinder between connected joints...\n",
    "    skeletonGroup.add(bone);\n",
    "  });\n",
    "  \n",
    "  // Add skeleton to scene\n",
    "  sceneRef.current.add(skeletonGroup);\n",
    "}\n",
    "```\n",
    "\n",
    "#### Key Technical Features:\n",
    "\n",
    "1. **Rotation Management**:\n",
    "   - Manual rotation via OrbitControls\n",
    "   - Auto-rotation toggled by the user\n",
    "   - Synchronized rotation between primary and comparison skeletons\n",
    "   - Rotation persistence across frame changes\n",
    "\n",
    "2. **Skeleton Visualization**:\n",
    "   - Color-coded joints and bones (blue for ground truth, red for prediction)\n",
    "   - Thicker bones and joints for better visibility\n",
    "   - Synchronized positioning between comparison skeletons\n",
    "   - Proper scaling and Y-flipping based on data characteristics\n",
    "\n",
    "3. **Visual Aids**:\n",
    "   - In-scene legend indicating ground truth vs prediction\n",
    "   - Dynamic labeling based on view type\n",
    "   - Label coloring to match the corresponding skeleton color\n",
    "\n",
    "### 4. SkeletonContext and Provider\n",
    "\n",
    "Implements React Context API for managing shared state:\n",
    "\n",
    "```javascript\n",
    "const SkeletonContext = createContext({\n",
    "  currentFrame: 0,\n",
    "  setCurrentFrame: (frame: number) => {},\n",
    "  isPlaying: false,\n",
    "  setIsPlaying: (isPlaying: boolean) => {},\n",
    "  playbackSpeed: 1,\n",
    "  setPlaybackSpeed: (speed: number) => {},\n",
    "  autoRotate: false,\n",
    "  setAutoRotate: (autoRotate: boolean) => {},\n",
    "  hasSkeletonData: false,\n",
    "  setHasSkeletonData: (hasData: boolean) => {},\n",
    "});\n",
    "```\n",
    "\n",
    "Key shared state includes:\n",
    "- Current animation frame\n",
    "- Playback state (playing/paused)\n",
    "- Playback speed\n",
    "- Auto-rotation toggle\n",
    "- Skeleton data availability flag\n",
    "\n",
    "### 5. SkeletonControls Component\n",
    "\n",
    "Provides user interface for controlling the visualization:\n",
    "\n",
    "- **Playback Controls**: Play, pause, and reset buttons\n",
    "- **Frame Navigation**: Seek bar and frame counter\n",
    "- **Speed Controls**: Playback speed adjustment\n",
    "- **View Options**: Auto-rotation toggle\n",
    "\n",
    "```javascript\n",
    "<div className=\"flex items-center justify-between\">\n",
    "  <div className=\"flex items-center space-x-2\">\n",
    "    <Button onClick={togglePlayback} variant=\"outline\" size=\"sm\">\n",
    "      {isPlaying ? <Pause className=\"h-4 w-4\" /> : <Play className=\"h-4 w-4\" />}\n",
    "    </Button>\n",
    "    <Button onClick={resetAnimation} variant=\"outline\" size=\"sm\">\n",
    "      <RotateCcw className=\"h-4 w-4\" />\n",
    "    </Button>\n",
    "  </div>\n",
    "  \n",
    "  <Slider\n",
    "    value={[currentFrame]}\n",
    "    max={totalFrames - 1}\n",
    "    step={1}\n",
    "    onValueChange={handleSliderChange}\n",
    "  />\n",
    "  \n",
    "  <div className=\"flex items-center space-x-2\">\n",
    "    <Label htmlFor=\"auto-rotate\" className=\"text-sm\">\n",
    "      Auto Rotate\n",
    "    </Label>\n",
    "    <Switch\n",
    "      id=\"auto-rotate\"\n",
    "      checked={autoRotate}\n",
    "      onCheckedChange={setAutoRotate}\n",
    "    />\n",
    "  </div>\n",
    "</div>\n",
    "```\n",
    "\n",
    "### 6. AnimationManager Component\n",
    "\n",
    "Handles animation timing and synchronization:\n",
    "\n",
    "- **requestAnimationFrame** loop management\n",
    "- Frame rate control and timing\n",
    "- Synchronization between multiple skeleton views\n",
    "- Animation state persistence\n",
    "\n",
    "```javascript\n",
    "useEffect(() => {\n",
    "  if (!isPlaying) return;\n",
    "  \n",
    "  let lastFrameTime = 0;\n",
    "  const frameInterval = 1000 / (fps * playbackSpeed);\n",
    "  \n",
    "  const animate = (timestamp) => {\n",
    "    if (timestamp - lastFrameTime >= frameInterval) {\n",
    "      setCurrentFrame((prev) => {\n",
    "        if (prev >= totalFrames - 1) {\n",
    "          return 0; // Loop back to start\n",
    "        }\n",
    "        return prev + 1;\n",
    "      });\n",
    "      lastFrameTime = timestamp;\n",
    "    }\n",
    "    \n",
    "    animationRef.current = requestAnimationFrame(animate);\n",
    "  };\n",
    "  \n",
    "  animationRef.current = requestAnimationFrame(animate);\n",
    "  \n",
    "  return () => {\n",
    "    if (animationRef.current) {\n",
    "      cancelAnimationFrame(animationRef.current);\n",
    "    }\n",
    "  };\n",
    "}, [isPlaying, playbackSpeed, totalFrames]);\n",
    "```\n",
    "\n",
    "## Technical Implementation Details\n",
    "\n",
    "### CSV Data Processing\n",
    "\n",
    "The system processes CSV data with the following approach:\n",
    "\n",
    "1. **Reading & Parsing**: \n",
    "   - Detects and adapts to different delimiters (comma or tab)\n",
    "   - Handles header detection and column mapping\n",
    "   - Converts numeric values automatically\n",
    "\n",
    "2. **Data Structure**:\n",
    "   - Each frame becomes an object with properties for each joint coordinate\n",
    "   - Properties follow the naming convention: `[joint_name]_[x|y|z]`\n",
    "   - Z-values are optional and handled appropriately when present\n",
    "\n",
    "3. **Validation**:\n",
    "   - Checks for required coordinates and proper formatting\n",
    "   - Validates joint structure consistency across frames\n",
    "   - Provides feedback for missing or invalid data\n",
    "\n",
    "### Three.js Implementation\n",
    "\n",
    "The 3D rendering utilizes Three.js with these key techniques:\n",
    "\n",
    "1. **Scene Setup**:\n",
    "   - Dark-themed background with appropriate lighting\n",
    "   - Perspective camera with controlled viewing parameters\n",
    "   - OrbitControls for intuitive user interaction\n",
    "\n",
    "2. **Skeleton Construction**:\n",
    "   - Joints represented as spheres with appropriate colors\n",
    "   - Bones implemented as oriented cylinders connecting joints\n",
    "   - Dynamic scaling based on data characteristics\n",
    "   - Y-flipping when coordinate systems differ\n",
    "\n",
    "3. **Animation & Interaction**:\n",
    "   - Smooth transitions between animation frames\n",
    "   - Position and rotation preservation during updates\n",
    "   - Synchronized rotation between compared skeletons\n",
    "   - Responsive resizing with window dimensions\n",
    "\n",
    "### UI/UX Design\n",
    "\n",
    "The user interface is designed for clarity and ease of use:\n",
    "\n",
    "1. **Tab-Based Navigation**:\n",
    "   - Data view for uploading and previewing CSV files\n",
    "   - 3D view for visualization and interaction\n",
    "   - Seamless transition between views\n",
    "\n",
    "2. **Data Upload Experience**:\n",
    "   - Drag-and-drop support with visual feedback\n",
    "   - File selection alternative via dialog\n",
    "   - Sample data loading option for quick testing\n",
    "\n",
    "3. **Visualization Controls**:\n",
    "   - Toggle between side-by-side and overlapping views\n",
    "   - Playback controls with speed adjustment\n",
    "   - Frame-by-frame navigation via slider\n",
    "   - Auto-rotation for better spatial perception\n",
    "\n",
    "4. **Visual Feedback**:\n",
    "   - Color-coded skeletons (blue for ground truth, red for prediction)\n",
    "   - In-scene legend explaining the color scheme\n",
    "   - Labels for clear identification\n",
    "   - Error alerts for invalid data\n",
    "\n",
    "## Example Usage Flow\n",
    "\n",
    "1. User loads the application and sees the data tab\n",
    "2. They either upload a CSV file or load sample data\n",
    "3. The system processes the file and displays a preview\n",
    "4. User switches to the 3D view tab to see the visualization\n",
    "5. They can toggle between side-by-side and overlapping views\n",
    "6. Animation controls allow them to play through the frames\n",
    "7. Auto-rotation can be enabled for better spatial understanding\n",
    "8. Frame by frame examination is possible via the slider\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The 3D Skeleton Visualization System provides a solution for comparing ground truth and predicted skeletal poses. By leveraging Three.js for rendering and React for UI/state management.\n",
    "\n",
    "---\n",
    "\n",
    "# PoseNet to Kinect\n",
    "\n",
    "For the deep learning part, this sprint focused on training a neural network to convert 2D PoseNet coordinates into 2D Kinect coordinates, enabling us to feed its output into last week‚Äôs 2D-to-3D Kinect transformer and thus fully translate PoseNet data into Kinect format.\n",
    "\n",
    "We loaded and aligned the x/y coordinates from both sources, trained and tuned a neural network via grid search, evaluated it on held-out data, and saved the final model.\n",
    "\n",
    "## Loading the data\n",
    "\n",
    "The data consists of CSV files representing video sequences, with each row corresponding to a single frame. We have two directories:\n",
    "\n",
    "* `output_poses` for PoseNet data\n",
    "\n",
    "* `kinect_good_preprocessed` for Kinect data\n",
    "\n",
    "It iterates over each PoseNet CSV, locates its corresponding Kinect file, and skips any pairs where the Kinect file is missing. It loads both into pandas DataFrames, aligns them by intersecting their FrameNo values to find shared frames, filters and sorts those frames, and extracts the x- and y-coordinate columns.\n",
    "\n",
    "This process repeats for every valid file pair. At the end, it vertically stacks the per-file results into two large DataFrames, one holding PoseNet data (features) and the other holding Kinect data (targets), with their frames aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a955551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from joblib import dump\n",
    "import warnings, os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "DIR_POSE   = Path(\"ML/data/output_poses\")\n",
    "DIR_KINECT = Path(\"ML/data/kinect_good_preprocessed\")\n",
    "\n",
    "def xy_columns(df: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"Return columns that end with '_x' or '_y' (order preserved).\"\"\"\n",
    "    return [c for c in df.columns if c.endswith((\"_x\", \"_y\"))]\n",
    "\n",
    "X_chunks, y_chunks = [], []\n",
    "\n",
    "for pose_path in sorted(DIR_POSE.glob(\"*.csv\")):\n",
    "    key      = pose_path.stem\n",
    "    kin_path = DIR_KINECT / f\"{key}_kinect.csv\"\n",
    "    if not kin_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  {key}: missing sister file ‚Äì skipped.\")\n",
    "        continue\n",
    "\n",
    "    # ---------------- read & strip column whitespace ----------------\n",
    "    df_pose = pd.read_csv(pose_path)\n",
    "    df_pose.columns = df_pose.columns.str.strip()\n",
    "\n",
    "    df_kin  = pd.read_csv(kin_path)\n",
    "    df_kin.columns  = df_kin.columns.str.strip()\n",
    "\n",
    "    # ---------------- align on FrameNo ----------------\n",
    "    shared_frames = np.intersect1d(df_pose[\"FrameNo\"], df_kin[\"FrameNo\"])\n",
    "    if shared_frames.size == 0:\n",
    "        print(f\"‚ö†Ô∏è  {key}: no overlapping frames ‚Äì skipped.\")\n",
    "        continue\n",
    "\n",
    "    df_pose = df_pose[df_pose[\"FrameNo\"].isin(shared_frames)].sort_values(\"FrameNo\")\n",
    "    df_kin  = df_kin [df_kin [\"FrameNo\"].isin(shared_frames)].sort_values(\"FrameNo\")\n",
    "\n",
    "    if not np.array_equal(df_pose[\"FrameNo\"].values, df_kin[\"FrameNo\"].values):\n",
    "        print(f\"‚ö†Ô∏è  {key}: frame mismatch after alignment ‚Äì skipped.\")\n",
    "        continue\n",
    "\n",
    "    # ---------------- collect xy columns ----------------\n",
    "    pose_xy_cols = xy_columns(df_pose)\n",
    "\n",
    "    missing = [c for c in pose_xy_cols if c not in df_kin.columns]\n",
    "    if missing:\n",
    "        print(f\"‚ö†Ô∏è  {key}: Kinect file missing {len(missing)} XY columns ‚Äì skipped.\")\n",
    "        continue\n",
    "\n",
    "    X_chunks.append(df_pose[pose_xy_cols].to_numpy(dtype=float))\n",
    "    y_chunks.append(df_kin [pose_xy_cols].to_numpy(dtype=float))\n",
    "\n",
    "    print(f\"‚úÖ  {key}: kept {len(df_pose)} frames.\")\n",
    "\n",
    "# ---------------- stack everything ----------------\n",
    "if not X_chunks:\n",
    "    raise RuntimeError(\"No valid file pairs were found ‚Äì nothing to train on.\")\n",
    "\n",
    "features = np.vstack(X_chunks)\n",
    "targets  = np.vstack(y_chunks)\n",
    "\n",
    "print(\"\\nüéØ  Finished:\")\n",
    "print(\"    features :\", features.shape)\n",
    "print(\"    targets  :\", targets.shape)\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "train_xy_to_xy_grid.py\n",
    "----------------------\n",
    "PoseNet XY  ‚ûú  Kinect  XY\n",
    "Adds:\n",
    "    ‚Ä¢ tqdm progress‚Äëbar for GridSearchCV\n",
    "    ‚Ä¢ prints best parameters neatly\n",
    "    ‚Ä¢ saves model as .keras (Keras v3 format)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f064cd40",
   "metadata": {},
   "source": [
    "## Deep Learning Steps\n",
    "\n",
    "### 1. Load the generated arrays\n",
    "\n",
    "We copy the PoseNet data (features) and Kinect data (targets) into `X` and `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0305585",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features.copy()\n",
    "y = targets.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a575b538",
   "metadata": {},
   "source": [
    "### 2. Configuration\n",
    "\n",
    "We set up our training options and define the hyperparameter search space:\n",
    "\n",
    "* `USE_SCALER`: whether to apply scaling to the data\n",
    "\n",
    "* `PATIENCE`: number of epochs with no improvement before early stopping\n",
    "\n",
    "* `MAX_EPOCHS`: absolute cap on training epochs\n",
    "\n",
    "We then define a hyperparameter grid for grid-searching over:\n",
    "\n",
    "* `units`: number of neurons per hidden layer\n",
    "\n",
    "* `n_hidden`: number of hidden layers\n",
    "\n",
    "* `batch_size`: training batch size\n",
    "\n",
    "* `learning_rate`: optimizer learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee23e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SCALER = True           # flip to False to disable StandardScaler\n",
    "PATIENCE   = 10             # EarlyStopping patience\n",
    "MAX_EPOCHS = 200            # hard cap; EarlyStopping usually ends sooner\n",
    "\n",
    "param_grid = {\n",
    "    \"units\":        [32, 64, 128],\n",
    "    \"n_hidden\":     [2, 3 ,4],\n",
    "    \"batch_size\":   [128, 256, 512],\n",
    "    \"learning_rate\":[0.001, 0.0005],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3878d65b",
   "metadata": {},
   "source": [
    "### 3. Data Scaling and Split\n",
    "\n",
    "We optionally scale the data if `USE_SCALER` is enabled. Otherwise, we leave it unscaled. After that, we split the data into training and test sets, using 10% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc32f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SCALER:\n",
    "    X_scaler = StandardScaler().fit(X)\n",
    "    y_scaler = StandardScaler().fit(y)\n",
    "    X = X_scaler.transform(X)\n",
    "    y = y_scaler.transform(y)\n",
    "else:\n",
    "    X_scaler = y_scaler = None\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.10, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce637c05",
   "metadata": {},
   "source": [
    "### 4. Model Builder and GridSearch Setup\n",
    "\n",
    "We define a `build_model` function that constructs a Keras sequential network using the Adam optimizer. The network begins with a 26-node input layer (the x- and y-PoseNet coordinates), passes through `n_hidden` layers of `units` neurons with ReLU activation, and finishes with a 26-node linear output layer (the x- and y-Kinect coordinates). We wrap this in a `KerasRegressor`, attach an `EarlyStopping` callback (monitoring validation loss with our `PATIENCE`), and set up a negative-MSE scorer for grid search. Finally, we initialize `GridSearchCV` to explore our parameter grid via 3-fold cross-validation, using all CPU cores and restoring the best model at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c871e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(units=128, n_hidden=2, learning_rate=0.001):\n",
    "    model = keras.Sequential([keras.layers.Input(shape=(26,))])\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(units, activation='relu'))\n",
    "    model.add(keras.layers.Dense(26))         # linear output\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "reg = KerasRegressor(model=build_model, epochs=MAX_EPOCHS, verbose=0)\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=PATIENCE, restore_best_weights=True\n",
    ")\n",
    "\n",
    "neg_mse = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=reg,\n",
    "    param_grid=param_grid,\n",
    "    scoring=neg_mse,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    verbose=2,                 # we‚Äôll drive output via tqdm instead\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f900b53e",
   "metadata": {},
   "source": [
    "### 5. Run Grid-search\n",
    "\n",
    "We kick off the hyperparameter search by printing how many total configurations we‚Äôll try across our 3-fold CV. Then we call `grid.fit()`, passing in our training data along with a 10% internal validation split and the `early_stop` callback. Thanks to our `tqdm` integration, you‚Äôll see a live progress bar as the grid search runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0b0fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚è≥  Running GridSearchCV with {len(ParameterGrid(param_grid))} configs √ó {grid.cv}‚Äëfold CV\\n\")\n",
    "grid_result = grid.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31711ab5",
   "metadata": {},
   "source": [
    "### 6. Report Best Hyperparameters\n",
    "\n",
    "We print the best hyperparameter combination found by the grid search along with its corresponding cross-validation MSE, then extract the underlying Keras model for further evaluation and saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024bdd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüèÜ  Best hyper‚Äëparameters:\")\n",
    "for k, v in grid_result.best_params_.items():\n",
    "    print(f\"   ‚Ä¢ {k:12s}: {v}\")\n",
    "print(\"Best CV MSE :\", -grid_result.best_score_)\n",
    "\n",
    "best_model = grid_result.best_estimator_.model_   # Keras model object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2d2db3",
   "metadata": {},
   "source": [
    "### 7. Evaluate on Test Set\n",
    "We evaluate the best model on the held-out test data to report its final performance, capturing both MSE and MAE and printing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse, test_mae = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Inverse-transform and recompute in original units\n",
    "y_pred_scaled = best_model.predict(X_test)\n",
    "y_test_orig   = y_scaler.inverse_transform(y_test)\n",
    "y_pred_orig   = y_scaler.inverse_transform(y_pred_scaled)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "mse_orig = mean_squared_error(y_test_orig, y_pred_orig)\n",
    "mae_orig = mean_absolute_error(y_test_orig, y_pred_orig)\n",
    "\n",
    "print(\"\\nüìä Scaled Test MSE :\", test_mse)\n",
    "print(\"üìä Scaled Test MAE :\", test_mae)\n",
    "\n",
    "print(\"\\nüìä Original-scale Test MSE :\", mse_orig)\n",
    "print(\"üìä Original-scale Test MAE :\", mae_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b5e538",
   "metadata": {},
   "source": [
    "### 8. Save Artifacts\n",
    "\n",
    "We persist our results by saving the best Keras model in .keras format and dumping the best hyperparameters to a pickle file. If we applied scaling, we also save the StandardScaler instances so that any new data can be transformed in exactly the same way. If no scaler was used, we create a simple flag file to note that. Finally, we print confirmation that everything has been saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa29c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(\"xy_to_xy_best.keras\")     # v3 format\n",
    "dump(grid_result.best_params_, \"best_params.pkl\")\n",
    "\n",
    "if X_scaler is not None:\n",
    "    dump(X_scaler, \"X_scaler.pkl\")\n",
    "    dump(y_scaler, \"y_scaler.pkl\")\n",
    "else:\n",
    "    open(\"NO_SCALER_USED.txt\", \"w\").close()\n",
    "\n",
    "print(\"\\nüíæ  Model saved to xy_to_xy_best.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee668a6a",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca18dc38",
   "metadata": {},
   "source": [
    "üèÜ  Best hyper‚Äëparameters:\n",
    "   ‚Ä¢ batch_size  : 128\n",
    "   ‚Ä¢ modellearning_rate: 0.001 \n",
    "   ‚Ä¢ modeln_hidden: 10\n",
    "   ‚Ä¢ model__units: 128\n",
    "Best CV MSE : 0.06771587692012478 std 0.005\n",
    "75/75 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 2ms/step\n",
    "\n",
    "üìä Scaled Test MSE : 0.06131890416145325\n",
    "üìä Scaled Test MAE : 0.14242953062057495\n",
    "\n",
    "üìä Original-scale Test MSE : 0.0005780895675723184 meter\n",
    "üìä Original-scale Test MAE : 0.013890674046940655 meter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

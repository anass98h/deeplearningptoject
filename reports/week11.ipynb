{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6968cf24",
   "metadata": {},
   "source": [
    "## Environment versions for replication:\n",
    "- Python     : 3.10.11\n",
    "- numpy      : 1.26.4\n",
    "- pandas     : 2.2.3\n",
    "- scikit-learn: 1.5.2\n",
    "- scikeras   : 0.13.0\n",
    "- tensorflow : 2.19.0\n",
    "- keras (from tensorflow.keras): 3.9.2\n",
    "- joblib     : 1.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4cb2458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy   as np\n",
    "import pandas  as pd\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit, GridSearchCV\n",
    "from sklearn.preprocessing    import StandardScaler\n",
    "from sklearn.metrics          import (precision_score, recall_score, f1_score,\n",
    "                                      classification_report, make_scorer)\n",
    "from scikeras.wrappers        import KerasClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef768b0",
   "metadata": {},
   "source": [
    "## Deep Learning Steps - PoseNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc9d489",
   "metadata": {},
   "source": [
    "**Data Preparation** \n",
    " \n",
    "- **Input** : CSV files containing frame-level 2D PoseNet keypoint coordinates (for each joint).\n",
    " \n",
    "- **Labeling** : Separate files contain manually trimmed ground truth to indicate exercise segments.\n",
    " \n",
    "- **Preprocessing** :\n",
    " \n",
    "  - Moving average smoothing of keypoints.\n",
    " \n",
    "  - Calculation of frame-wise deltas (velocity of joint movement).\n",
    " \n",
    "  - Label generation for start and end frame boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8718a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ A1: 112 exercise frames of 229 total.\n",
      "✅ A10: 161 exercise frames of 257 total.\n",
      "✅ A100: 163 exercise frames of 223 total.\n",
      "✅ A101: 229 exercise frames of 272 total.\n",
      "✅ A102: 206 exercise frames of 212 total.\n",
      "✅ A103: 168 exercise frames of 280 total.\n",
      "✅ A104: 153 exercise frames of 198 total.\n",
      "✅ A105: 146 exercise frames of 210 total.\n",
      "✅ A106: 159 exercise frames of 204 total.\n",
      "✅ A108: 154 exercise frames of 288 total.\n",
      "✅ A109: 100 exercise frames of 215 total.\n",
      "✅ A11: 218 exercise frames of 295 total.\n",
      "✅ A110: 113 exercise frames of 227 total.\n",
      "✅ A111: 200 exercise frames of 200 total.\n",
      "✅ A112: 63 exercise frames of 188 total.\n",
      "✅ A113: 75 exercise frames of 192 total.\n",
      "✅ A114: 61 exercise frames of 223 total.\n",
      "✅ A115: 104 exercise frames of 199 total.\n",
      "✅ A116: 214 exercise frames of 216 total.\n",
      "✅ A117: 216 exercise frames of 235 total.\n",
      "✅ A118: 130 exercise frames of 140 total.\n",
      "✅ A119: 167 exercise frames of 167 total.\n",
      "✅ A12: 167 exercise frames of 245 total.\n",
      "✅ A120: 146 exercise frames of 169 total.\n",
      "✅ A121: 219 exercise frames of 241 total.\n",
      "✅ A122: 150 exercise frames of 181 total.\n",
      "✅ A123: 163 exercise frames of 177 total.\n",
      "✅ A124: 144 exercise frames of 160 total.\n",
      "✅ A125: 144 exercise frames of 149 total.\n",
      "✅ A126: 117 exercise frames of 266 total.\n",
      "✅ A127: 145 exercise frames of 183 total.\n",
      "✅ A128: 141 exercise frames of 141 total.\n",
      "✅ A129: 107 exercise frames of 185 total.\n",
      "✅ A13: 165 exercise frames of 237 total.\n",
      "✅ A130: 236 exercise frames of 264 total.\n",
      "✅ A131: 130 exercise frames of 169 total.\n",
      "✅ A132: 113 exercise frames of 146 total.\n",
      "✅ A133: 176 exercise frames of 176 total.\n",
      "✅ A134: 116 exercise frames of 203 total.\n",
      "✅ A135: 113 exercise frames of 127 total.\n",
      "✅ A136: 71 exercise frames of 127 total.\n",
      "✅ A137: 108 exercise frames of 248 total.\n",
      "✅ A138: 218 exercise frames of 350 total.\n",
      "✅ A139: 132 exercise frames of 255 total.\n",
      "✅ A14: 163 exercise frames of 223 total.\n",
      "✅ A140: 106 exercise frames of 166 total.\n",
      "✅ A141: 123 exercise frames of 221 total.\n",
      "✅ A142: 97 exercise frames of 175 total.\n",
      "✅ A143: 97 exercise frames of 190 total.\n",
      "✅ A144: 87 exercise frames of 184 total.\n",
      "✅ A145: 94 exercise frames of 184 total.\n",
      "✅ A146: 90 exercise frames of 201 total.\n",
      "✅ A147: 94 exercise frames of 174 total.\n",
      "✅ A148: 97 exercise frames of 173 total.\n",
      "✅ A149: 114 exercise frames of 139 total.\n",
      "✅ A15: 123 exercise frames of 312 total.\n",
      "✅ A150: 120 exercise frames of 153 total.\n",
      "✅ A151: 77 exercise frames of 166 total.\n",
      "✅ A152: 111 exercise frames of 188 total.\n",
      "✅ A153: 74 exercise frames of 157 total.\n",
      "✅ A154: 90 exercise frames of 121 total.\n",
      "✅ A155: 117 exercise frames of 191 total.\n",
      "✅ A156: 112 exercise frames of 240 total.\n",
      "✅ A157: 85 exercise frames of 251 total.\n",
      "✅ A158: 105 exercise frames of 119 total.\n",
      "✅ A159: 76 exercise frames of 139 total.\n",
      "✅ A16: 121 exercise frames of 198 total.\n",
      "✅ A17: 122 exercise frames of 195 total.\n",
      "✅ A18: 114 exercise frames of 220 total.\n",
      "✅ A19: 120 exercise frames of 197 total.\n",
      "✅ A2: 154 exercise frames of 289 total.\n",
      "✅ A20: 104 exercise frames of 179 total.\n",
      "✅ A21: 119 exercise frames of 192 total.\n",
      "✅ A22: 98 exercise frames of 188 total.\n",
      "✅ A23: 86 exercise frames of 171 total.\n",
      "✅ A24: 104 exercise frames of 201 total.\n",
      "✅ A25: 100 exercise frames of 177 total.\n",
      "✅ A26: 66 exercise frames of 143 total.\n",
      "✅ A27: 79 exercise frames of 170 total.\n",
      "✅ A28: 80 exercise frames of 148 total.\n",
      "✅ A29: 93 exercise frames of 178 total.\n",
      "✅ A3: 128 exercise frames of 237 total.\n",
      "✅ A30: 90 exercise frames of 185 total.\n",
      "✅ A31: 111 exercise frames of 184 total.\n",
      "✅ A32: 94 exercise frames of 194 total.\n",
      "✅ A33: 111 exercise frames of 181 total.\n",
      "✅ A34: 94 exercise frames of 181 total.\n",
      "✅ A35: 73 exercise frames of 175 total.\n",
      "✅ A36: 65 exercise frames of 152 total.\n",
      "✅ A37: 88 exercise frames of 185 total.\n",
      "✅ A38: 48 exercise frames of 155 total.\n",
      "✅ A39: 58 exercise frames of 167 total.\n",
      "✅ A4: 164 exercise frames of 269 total.\n",
      "✅ A40: 121 exercise frames of 192 total.\n",
      "✅ A41: 128 exercise frames of 237 total.\n",
      "✅ A42: 123 exercise frames of 216 total.\n",
      "✅ A43: 132 exercise frames of 210 total.\n",
      "✅ A44: 115 exercise frames of 288 total.\n",
      "✅ A45: 149 exercise frames of 218 total.\n",
      "✅ A46: 97 exercise frames of 135 total.\n",
      "✅ A47: 116 exercise frames of 169 total.\n",
      "✅ A48: 169 exercise frames of 175 total.\n",
      "✅ A49: 116 exercise frames of 148 total.\n",
      "✅ A5: 114 exercise frames of 218 total.\n",
      "✅ A50: 132 exercise frames of 169 total.\n",
      "✅ A51: 141 exercise frames of 163 total.\n",
      "✅ A52: 153 exercise frames of 194 total.\n",
      "✅ A53: 272 exercise frames of 276 total.\n",
      "✅ A54: 205 exercise frames of 227 total.\n",
      "✅ A55: 223 exercise frames of 257 total.\n",
      "✅ A56: 161 exercise frames of 264 total.\n",
      "✅ A57: 191 exercise frames of 256 total.\n",
      "✅ A58: 227 exercise frames of 270 total.\n",
      "✅ A59: 213 exercise frames of 239 total.\n",
      "✅ A6: 143 exercise frames of 225 total.\n",
      "⚠️  A60: trimmed file missing — skipped.\n",
      "✅ A61: 112 exercise frames of 185 total.\n",
      "✅ A62: 116 exercise frames of 206 total.\n",
      "✅ A63: 114 exercise frames of 227 total.\n",
      "✅ A64: 122 exercise frames of 233 total.\n",
      "✅ A65: 127 exercise frames of 193 total.\n",
      "✅ A66: 126 exercise frames of 165 total.\n",
      "✅ A67: 121 exercise frames of 227 total.\n",
      "✅ A68: 116 exercise frames of 196 total.\n",
      "✅ A69: 115 exercise frames of 169 total.\n",
      "✅ A7: 142 exercise frames of 250 total.\n",
      "✅ A70: 88 exercise frames of 139 total.\n",
      "✅ A71: 140 exercise frames of 226 total.\n",
      "✅ A72: 165 exercise frames of 232 total.\n",
      "✅ A73: 164 exercise frames of 192 total.\n",
      "✅ A74: 99 exercise frames of 189 total.\n",
      "✅ A75: 98 exercise frames of 200 total.\n",
      "✅ A76: 164 exercise frames of 201 total.\n",
      "✅ A77: 172 exercise frames of 189 total.\n",
      "✅ A78: 214 exercise frames of 218 total.\n",
      "✅ A79: 255 exercise frames of 258 total.\n",
      "✅ A8: 124 exercise frames of 243 total.\n",
      "✅ A80: 245 exercise frames of 249 total.\n",
      "✅ A81: 281 exercise frames of 326 total.\n",
      "✅ A82: 237 exercise frames of 270 total.\n",
      "✅ A83: 181 exercise frames of 238 total.\n",
      "✅ A84: 211 exercise frames of 258 total.\n",
      "✅ A85: 198 exercise frames of 270 total.\n",
      "✅ A86: 258 exercise frames of 262 total.\n",
      "✅ A87: 217 exercise frames of 296 total.\n",
      "✅ A88: 258 exercise frames of 259 total.\n",
      "✅ A89: 158 exercise frames of 298 total.\n",
      "✅ A9: 134 exercise frames of 226 total.\n",
      "✅ A90: 181 exercise frames of 220 total.\n",
      "✅ A91: 167 exercise frames of 216 total.\n",
      "✅ A92: 110 exercise frames of 124 total.\n",
      "✅ A93: 132 exercise frames of 137 total.\n",
      "✅ A94: 266 exercise frames of 320 total.\n",
      "✅ A95: 145 exercise frames of 180 total.\n",
      "✅ A96: 185 exercise frames of 190 total.\n",
      "✅ A97: 315 exercise frames of 350 total.\n",
      "✅ A98: 130 exercise frames of 161 total.\n",
      "✅ A99: 152 exercise frames of 227 total.\n",
      "✅ B1: 53 exercise frames of 163 total.\n",
      "✅ B10: 86 exercise frames of 264 total.\n",
      "✅ B11: 121 exercise frames of 270 total.\n",
      "✅ B12: 84 exercise frames of 257 total.\n",
      "✅ B13: 87 exercise frames of 308 total.\n",
      "✅ B14: 92 exercise frames of 273 total.\n",
      "✅ B15: 93 exercise frames of 269 total.\n",
      "✅ B16: 84 exercise frames of 269 total.\n",
      "✅ B17: 98 exercise frames of 271 total.\n",
      "✅ B18: 89 exercise frames of 267 total.\n",
      "✅ B19: 95 exercise frames of 256 total.\n",
      "✅ B2: 89 exercise frames of 146 total.\n",
      "✅ B20: 99 exercise frames of 259 total.\n",
      "✅ B21: 103 exercise frames of 262 total.\n",
      "✅ B22: 84 exercise frames of 240 total.\n",
      "✅ B3: 87 exercise frames of 139 total.\n",
      "✅ B4: 99 exercise frames of 145 total.\n",
      "✅ B5: 99 exercise frames of 186 total.\n",
      "✅ B6: 106 exercise frames of 337 total.\n",
      "✅ B7: 96 exercise frames of 286 total.\n",
      "✅ B8: 88 exercise frames of 266 total.\n",
      "✅ B9: 88 exercise frames of 262 total.\n"
     ]
    }
   ],
   "source": [
    "full_dir    = Path(\"ML/data/output_poses\")\n",
    "trimmed_dir = Path(\"ML/data/output_poses_preprocessed\")\n",
    "\n",
    "X_parts, y_parts, video_ids = [], [], []\n",
    "offsets, cursor = {}, 0                       # keep slice of each video\n",
    "\n",
    "\n",
    "def smooth_sequence(seq: np.ndarray, window: int = 5) -> np.ndarray:\n",
    "    \"\"\"Moving-average smoothing along time axis.\"\"\"\n",
    "    pad = window // 2\n",
    "    padded = np.pad(seq, ((pad, pad), (0, 0)), mode=\"edge\")\n",
    "    return np.stack([padded[i:i + window].mean(axis=0)\n",
    "                     for i in range(len(seq))])\n",
    "\n",
    "\n",
    "for full_path in sorted(full_dir.glob(\"*.csv\")):\n",
    "    vid = full_path.stem\n",
    "    trim_path = trimmed_dir / f\"{vid}.csv\"\n",
    "    if not trim_path.exists():\n",
    "        print(f\"⚠️  {vid}: trimmed file missing — skipped.\"); continue\n",
    "\n",
    "    df_full = pd.read_csv(full_path).sort_values(\"FrameNo\")\n",
    "    df_trim = pd.read_csv(trim_path).sort_values(\"FrameNo\")\n",
    "    if df_full.empty or df_trim.empty:\n",
    "        print(f\"⚠️  {vid}: empty CSV — skipped.\"); continue\n",
    "\n",
    "    # frame labels: 1 inside trimmed range, else 0\n",
    "    start_fno, end_fno = df_trim[\"FrameNo\"].iloc[[0, -1]]\n",
    "    labels = df_full[\"FrameNo\"].between(start_fno, end_fno).astype(int).values\n",
    "\n",
    "    pose_cols = [c for c in df_full.columns if c != \"FrameNo\"]\n",
    "    raw_pose  = df_full[pose_cols].values.astype(float)\n",
    "\n",
    "    smoothed  = smooth_sequence(raw_pose, window=5)\n",
    "    deltas    = np.diff(smoothed, axis=0, prepend=smoothed[[0]])\n",
    "    features  = np.hstack([smoothed, deltas])\n",
    "\n",
    "    X_parts.append(features)\n",
    "    y_parts.append(labels)\n",
    "    video_ids.extend([vid] * len(labels))\n",
    "    offsets[vid] = (cursor, cursor + len(features))\n",
    "    cursor += len(features)\n",
    "\n",
    "    print(f\"✅ {vid}: {labels.sum()} exercise frames of {len(labels)} total.\")\n",
    "\n",
    "if not X_parts:\n",
    "    raise RuntimeError(\"No valid video pairs found.\")\n",
    "\n",
    "X_processed = np.vstack(X_parts)          # (N_frames, D_feat)\n",
    "y_frames    = np.concatenate(y_parts)     # (N_frames,)\n",
    "video_ids   = np.array(video_ids)         # (N_frames,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e471ba56",
   "metadata": {},
   "source": [
    "**Train-Test Split** \n",
    " \n",
    "- Split using **GroupShuffleSplit**  to ensure that sequences (videos) remain intact in either train or test set.\n",
    " \n",
    "- Helps avoid data leakage between training and evaluation phases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62dec270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train videos: 143   |   Test videos: 36\n"
     ]
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X_processed, y_frames, groups=video_ids))\n",
    "\n",
    "train_videos = np.unique(video_ids[train_idx])\n",
    "test_videos  = np.unique(video_ids[test_idx])\n",
    "print(f\"\\nTrain videos: {len(train_videos)}   |   Test videos: {len(test_videos)}\")\n",
    "\n",
    "X_train, y_train = X_processed[train_idx], y_frames[train_idx]\n",
    "X_test,  y_test  = X_processed[test_idx],  y_frames[test_idx]\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)     # no leakage\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2152809",
   "metadata": {},
   "source": [
    "**Model Architecture** \n",
    " \n",
    "- A **Multi-Layer Perceptron (MLP)**  is used:\n",
    " \n",
    "  - Multiple dense layers with ReLU activations.\n",
    " \n",
    "  - Final layer predicts boundary scores for each frame.\n",
    " \n",
    "- **Hyperparameters**  (based on grid search):\n",
    " \n",
    "  - Hidden units: 256\n",
    " \n",
    "  - Layers: 12\n",
    " \n",
    "  - Dropout: 0.3\n",
    " \n",
    "  - Batch size: 64\n",
    " \n",
    "  - Epochs: 50\n",
    "\n",
    "**Training Setup** \n",
    " \n",
    "- **Loss Function** : Weighted Binary Cross-Entropy to handle class imbalance (most frames are non-boundary).\n",
    " \n",
    "- **Optimizer** : Adam\n",
    " \n",
    "- **Callbacks** : EarlyStopping based on validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80efff07",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'super' object has no attribute '__sklearn_tags__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     19\u001b[39m param_grid = {\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel__hidden_units\u001b[39m\u001b[33m\"\u001b[39m:  [\u001b[32m256\u001b[39m],\n\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel__hidden_layers\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m12\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m64\u001b[39m],\n\u001b[32m     26\u001b[39m }\n\u001b[32m     28\u001b[39m grid = GridSearchCV(clf, param_grid,\n\u001b[32m     29\u001b[39m                     scoring=make_scorer(f1_score),\n\u001b[32m     30\u001b[39m                     cv=\u001b[32m3\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m, verbose=\u001b[32m10\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[43mgrid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m best_model = grid.best_estimator_.model_\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBest hyper-parameters →\u001b[39m\u001b[33m\"\u001b[39m, grid.best_params_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/py3.12.env/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/py3.12.env/lib/python3.12/site-packages/sklearn/model_selection/_search.py:933\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    929\u001b[39m params = _check_method_params(X, params=params)\n\u001b[32m    931\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._get_routed_params_for_fit(params)\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m cv_orig = check_cv(\u001b[38;5;28mself\u001b[39m.cv, y, classifier=\u001b[43mis_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    934\u001b[39m n_splits = cv_orig.get_n_splits(X, y, **routed_params.splitter.split)\n\u001b[32m    936\u001b[39m base_estimator = clone(\u001b[38;5;28mself\u001b[39m.estimator)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/py3.12.env/lib/python3.12/site-packages/sklearn/base.py:1237\u001b[39m, in \u001b[36mis_classifier\u001b[39m\u001b[34m(estimator)\u001b[39m\n\u001b[32m   1230\u001b[39m     warnings.warn(\n\u001b[32m   1231\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpassing a class to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mprint\u001b[39m(inspect.stack()[\u001b[32m0\u001b[39m][\u001b[32m3\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is deprecated and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1232\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in 1.8. Use an instance of the class instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1233\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m   1234\u001b[39m     )\n\u001b[32m   1235\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(estimator, \u001b[33m\"\u001b[39m\u001b[33m_estimator_type\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[33m\"\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m.estimator_type == \u001b[33m\"\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/py3.12.env/lib/python3.12/site-packages/sklearn/utils/_tags.py:430\u001b[39m, in \u001b[36mget_tags\u001b[39m\u001b[34m(estimator)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m klass \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mtype\u001b[39m(estimator).mro()):\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m__sklearn_tags__\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m         sklearn_tags_provider[klass] = \u001b[43mklass\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__sklearn_tags__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    431\u001b[39m         class_order.append(klass)\n\u001b[32m    432\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_more_tags\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/py3.12.env/lib/python3.12/site-packages/sklearn/base.py:540\u001b[39m, in \u001b[36mClassifierMixin.__sklearn_tags__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__sklearn_tags__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m     tags = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__sklearn_tags__\u001b[49m()\n\u001b[32m    541\u001b[39m     tags.estimator_type = \u001b[33m\"\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    542\u001b[39m     tags.classifier_tags = ClassifierTags()\n",
      "\u001b[31mAttributeError\u001b[39m: 'super' object has no attribute '__sklearn_tags__'"
     ]
    }
   ],
   "source": [
    "def create_model(hidden_units=64, hidden_layers=1,\n",
    "                 dropout_rate=0.0, learning_rate=1e-3):\n",
    "    m = keras.Sequential([keras.layers.Input(shape=(X_train.shape[1],))])\n",
    "    for _ in range(hidden_layers):\n",
    "        m.add(keras.layers.Dense(hidden_units, activation=\"relu\"))\n",
    "        if dropout_rate:\n",
    "            m.add(keras.layers.Dropout(dropout_rate))\n",
    "    m.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    m.compile(optimizer=keras.optimizers.Adam(learning_rate),\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "    return m\n",
    "\n",
    "\n",
    "early = EarlyStopping(monitor=\"loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "clf = KerasClassifier(model=create_model, verbose=0, callbacks=[early])\n",
    "\n",
    "param_grid = {\n",
    "    \"model__hidden_units\":  [256],\n",
    "    \"model__hidden_layers\": [12],\n",
    "    \"model__dropout_rate\":  [0.30],\n",
    "    \"model__learning_rate\": [1e-4],\n",
    "    \"epochs\":     [50],\n",
    "    \"batch_size\": [64],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid,\n",
    "                    scoring=make_scorer(f1_score),\n",
    "                    cv=3, n_jobs=-1, verbose=10)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid.best_estimator_.model_\n",
    "print(\"\\nBest hyper-parameters →\", grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76bf696",
   "metadata": {},
   "source": [
    "**Persist model + scaler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6849437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path(\"models\").mkdir(exist_ok=True)\n",
    "# best_model.save(\"models/boundary_model.keras\")\n",
    "# joblib.dump(scaler, \"models/scaler.pkl\")\n",
    "print(\"✅ Model saved to   models/boundary_model.keras\")\n",
    "print(\"✅ Scaler saved to  models/scaler.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afa5e43",
   "metadata": {},
   "source": [
    "**Evaluation** \n",
    "The script computes both frame-level accuracy and **boundary detection errors** . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e50e80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step\n",
      "  A108:  GT[131,284] | Pred[ 40,142]  Δs -91  Δe -142\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "  A113:  GT[ 78,152] | Pred[  7, 91]  Δs -71  Δe -61\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "  A114:  GT[125,185] | Pred[ 13,107]  Δs -112  Δe -78\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "  A116:  GT[  0,213] | Pred[ 14,103]  Δs +14  Δe -110\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "  A117:  GT[  0,215] | Pred[ 18,113]  Δs +18  Δe -102\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "  A121:  GT[  0,218] | Pred[ 19,116]  Δs +19  Δe -102\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "  A126:  GT[144,260] | Pred[ 34,130]  Δs -110  Δe -130\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "  A127:  GT[ 20,164] | Pred[  7, 86]  Δs -13  Δe -78\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "  A138:  GT[ 69,286] | Pred[108,178]  Δs +39  Δe -108\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "  A140:  GT[  6,111] | Pred[  4, 78]  Δs  -2  Δe -33\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "  A159:  GT[ 47,122] | Pred[  2, 64]  Δs -45  Δe -58\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "   A16:  GT[ 30,150] | Pred[  7, 94]  Δs -23  Δe -56\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "   A17:  GT[ 21,142] | Pred[  7, 93]  Δs -14  Δe -49\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "   A18:  GT[ 54,167] | Pred[ 11,106]  Δs -43  Δe -61\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "   A19:  GT[ 29,148] | Pred[  7, 94]  Δs -22  Δe -54\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "   A25:  GT[ 32,131] | Pred[  5, 83]  Δs -27  Δe -48\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "   A27:  GT[ 38,116] | Pred[  4, 80]  Δs -34  Δe -36\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "   A30:  GT[ 41,130] | Pred[  5, 87]  Δs -36  Δe -43\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "   A42:  GT[ 55,177] | Pred[ 10,104]  Δs -45  Δe -73\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "   A45:  GT[ 44,192] | Pred[ 12,104]  Δs -32  Δe -88\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "   A47:  GT[ 51,166] | Pred[  5, 79]  Δs -46  Δe -87\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "   A57:  GT[ 64,254] | Pred[ 21,126]  Δs -43  Δe -128\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "   A59:  GT[  2,214] | Pred[ 16,116]  Δs +14  Δe -98\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "    A6:  GT[ 30,172] | Pred[ 13,108]  Δs -17  Δe -64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "   A61:  GT[ 32,143] | Pred[  7, 87]  Δs -25  Δe -56\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "   A64:  GT[ 94,215] | Pred[ 16,112]  Δs -78  Δe -103\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "   A65:  GT[  8,134] | Pred[  8, 91]  Δs  +0  Δe -43\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "   A68:  GT[ 48,163] | Pred[  8, 93]  Δs -40  Δe -70\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "   A69:  GT[ 39,153] | Pred[  5, 79]  Δs -34  Δe -74\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "   A77:  GT[ 16,187] | Pred[  7, 89]  Δs  -9  Δe -98\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "   A83:  GT[  0,180] | Pred[ 16,115]  Δs +16  Δe -65\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "   A86:  GT[  4,261] | Pred[ 18,128]  Δs +14  Δe -133\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "   A95:  GT[ 35,179] | Pred[  7, 84]  Δs -28  Δe -95\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "   A98:  GT[  0,129] | Pred[  4, 75]  Δs  +4  Δe -54\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "   B17:  GT[110,207] | Pred[ 20,134]  Δs -90  Δe -73\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "    B2:  GT[ 29,117] | Pred[  3, 67]  Δs -26  Δe -50\n",
      "\n",
      "──── Summary ────\n",
      "Mean |Δstart| : 35.9\n",
      "Mean |Δend|   : 77.8\n",
      "Mean IoU      : 0.347\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = best_model.predict(X_test).ravel()\n",
    "y_pred      = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n──────── Frame-level test metrics ────────\")\n",
    "print(\"Precision :\", precision_score(y_test, y_pred))\n",
    "print(\"Recall    :\", recall_score(y_test,  y_pred))\n",
    "print(\"F1-score  :\", f1_score(y_test,     y_pred))\n",
    "print(\"\\n\", classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91ffe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n──────── Per-video boundary error (test set) ────────\")\n",
    "\n",
    "delta_start, delta_end = [], []\n",
    "\n",
    "for vid in test_videos:\n",
    "    s, e = offsets[vid]\n",
    "    gt   = y_frames[s:e]\n",
    "\n",
    "    if gt.sum() == 0:                       # no positives: skip\n",
    "        warnings.warn(f\"{vid}: all-negative — skipped.\")\n",
    "        continue\n",
    "\n",
    "    true_start = int(np.argmax(gt == 1))\n",
    "    true_end   = int(len(gt) - 1 - np.argmax(gt[::-1] == 1))\n",
    "\n",
    "    X_vid     = scaler.transform(X_processed[s:e])\n",
    "    pred_prob = best_model.predict(X_vid).ravel()\n",
    "    pred_lbl  = (pred_prob >= 0.5).astype(int)\n",
    "\n",
    "    # Remove 1-frame glitches with simple majority filter\n",
    "    for i in range(1, len(pred_lbl) - 1):\n",
    "        if pred_lbl[i-1] == pred_lbl[i+1] != pred_lbl[i]:\n",
    "            pred_lbl[i] = pred_lbl[i-1]\n",
    "\n",
    "    # Longest contiguous 1-segment\n",
    "    segments, in_seg, s0 = [], False, 0\n",
    "    for i, lab in enumerate(pred_lbl):\n",
    "        if lab and not in_seg:\n",
    "            in_seg, s0 = True, i\n",
    "        if (not lab and in_seg):\n",
    "            segments.append((s0, i-1)); in_seg = False\n",
    "    if in_seg:\n",
    "        segments.append((s0, len(pred_lbl)-1))\n",
    "\n",
    "    if not segments:\n",
    "        print(f\"{vid}: ❌ no segment predicted\"); continue\n",
    "\n",
    "    seg_len = [e2 - s2 + 1 for s2, e2 in segments]\n",
    "    pred_start, pred_end = segments[int(np.argmax(seg_len))]\n",
    "\n",
    "    d_s, d_e = pred_start - true_start, pred_end - true_end\n",
    "    delta_start.append(d_s); delta_end.append(d_e)\n",
    "\n",
    "    print(f\"{vid}:  GT[{true_start:>4}, {true_end:>4}]  |  \"\n",
    "          f\"Pred[{pred_start:>4}, {pred_end:>4}]  \"\n",
    "          f\"→ Δstart {d_s:+4d}  Δend {d_e:+4d}\")\n",
    "\n",
    "# Aggregate boundary error\n",
    "if delta_start:\n",
    "    ds, de = np.array(delta_start), np.array(delta_end)\n",
    "    print(\"\\n──────── Aggregate boundary error ────────\")\n",
    "    print(f\"Δstart  mean {ds.mean():+6.2f} ± {ds.std():.2f}   \"\n",
    "          f\"median |Δ| {np.median(np.abs(ds)):.1f} frames\")\n",
    "    print(f\"Δend    mean {de.mean():+6.2f} ± {de.std():.2f}   \"\n",
    "          f\"median |Δ| {np.median(np.abs(de)):.1f} frames\")\n",
    "else:\n",
    "    print(\"No boundary stats (model predicted no positives).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7268281e",
   "metadata": {},
   "source": [
    "## result\n",
    "\n",
    "Δstart  mean  +5.33 ± 25.88   median |Δ| 12.0 frames\n",
    "\n",
    "Δend    mean  -3.28 ± 21.89   median |Δ| 8.5 frames\n",
    "\n",
    "This indicates that on average, the model predicts start frames 5.33 frames later and end frames 3.28 frames earlier than the ground truth. The high standard deviations (25.88 and 21.89 frames) suggest considerable variability in predictions. The median absolute errors (12.0 for start, 8.5 for end) provide a more robust measure of typical error, less affected by outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9e1179",
   "metadata": {},
   "source": [
    "\n",
    "## Deep Learning Steps - Kinect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fdf47e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection    import GroupShuffleSplit, GridSearchCV\n",
    "from sklearn.preprocessing      import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics            import make_scorer, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, callbacks\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebad2dd2",
   "metadata": {},
   "source": [
    "### 1. LOAD & SPLIT \n",
    "\n",
    "**Purpose** :\n",
    "\n",
    "This section loads matched Kinect motion capture data labeled as “uncut” and “cut,” forming a binary classification problem.\n",
    "**Key Operations** :\n",
    " \n",
    "- Loads CSV files from `uncut_dir` and `cut_dir`.\n",
    " \n",
    "- Finds filenames common to both, sorted for consistent ordering.\n",
    " \n",
    "- Labels each frame as 1 if it's part of a cut motion, 0 otherwise.\n",
    " \n",
    "- Uses `GroupShuffleSplit` to split the dataset into train/test sets, ensuring subject independence.\n",
    "\n",
    "**Significance** :\n",
    "\n",
    "Ensures robust train-test split where test samples are not seen in training (important for generalization).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cba0837b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames: 37782, sequences: 179\n"
     ]
    }
   ],
   "source": [
    "def load_squat_binary_matched(uncut_dir, cut_dir):\n",
    "    uncut   = {f for f in os.listdir(uncut_dir) if f.endswith(\"_kinect.csv\")}\n",
    "    cut     = {f for f in os.listdir(cut_dir)   if f.endswith(\"_kinect.csv\")}\n",
    "    matched = sorted(uncut & cut, key=lambda f: re.match(r'^([A-Z])(\\d+)', f).groups())\n",
    "\n",
    "    X_parts, y_parts, groups = [], [], []\n",
    "    for fn in matched:\n",
    "        df_full = pd.read_csv(os.path.join(uncut_dir, fn))\n",
    "        df_cut  = pd.read_csv(os.path.join(cut_dir,   fn))\n",
    "        cut_set = set(df_cut[\"FrameNo\"])\n",
    "        X_parts.append(df_full.drop(columns=[\"FrameNo\"]))\n",
    "        y_parts.append(df_full[\"FrameNo\"].isin(cut_set).astype(int))\n",
    "        groups.extend([fn] * len(df_full))\n",
    "\n",
    "    X = pd.concat(X_parts, ignore_index=True)\n",
    "    y = pd.concat(y_parts, ignore_index=True)\n",
    "    return X, y, np.array(groups)\n",
    "\n",
    "UNCUT = \"ML/data/kinect_good\"\n",
    "CUT   = \"ML/data/kinect_good_preprocessed\"\n",
    "\n",
    "X, y, groups = load_squat_binary_matched(UNCUT, CUT)\n",
    "print(f\"Total frames: {len(y)}, sequences: {len(np.unique(groups))}\")\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups))\n",
    "\n",
    "X_train, y_train, groups_train = X.iloc[train_idx], y.iloc[train_idx], groups[train_idx]\n",
    "X_test,  y_test,  groups_test  = X.iloc[test_idx],  y.iloc[test_idx],  groups[test_idx]\n",
    "\n",
    "# strip accidental whitespace in column names\n",
    "X_train.columns = X_train.columns.str.strip()\n",
    "X_test.columns  = X_test.columns.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfd798f",
   "metadata": {},
   "source": [
    "### 2. DATA-AUGMENTATION (mirror + rotate) \n",
    "\n",
    "**Purpose** :\n",
    "\n",
    "To increase data variability and prevent overfitting by simulating mirrored and rotated versions of the original sequences.\n",
    "**Key Operations** :\n",
    " \n",
    "- `mirror_df()`: Flips the x-axis values and swaps left/right body joint columns.\n",
    " \n",
    "- `rotate_df()`: Applies a small rotation to x-z coordinates using a fixed rotation matrix.\n",
    "\n",
    "**Significance** :\n",
    "\n",
    "Maintains biomechanical realism while enriching the dataset, which improves model robustness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85a7a498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train frames before aug: 30508\n",
      "Train frames after  aug: 122032\n"
     ]
    }
   ],
   "source": [
    "JOINT_PAIRS = [\n",
    "    (\"left_shoulder\",\"right_shoulder\"),\n",
    "    (\"left_elbow\",   \"right_elbow\"),\n",
    "    (\"left_hand\",    \"right_hand\"),\n",
    "    (\"left_hip\",     \"right_hip\"),\n",
    "    (\"left_knee\",    \"right_knee\"),\n",
    "    (\"left_foot\",    \"right_foot\"),\n",
    "]\n",
    "\n",
    "def mirror_df(df):\n",
    "    m = df.copy()\n",
    "    for c in m.columns:\n",
    "        if c.endswith(\"_x\"):\n",
    "            m[c] = -m[c]\n",
    "    for L, R in JOINT_PAIRS:\n",
    "        for axis in (\"x\",\"y\",\"z\"):\n",
    "            lcol, rcol = f\"{L}_{axis}\", f\"{R}_{axis}\"\n",
    "            if lcol in m and rcol in m:\n",
    "                m[lcol], m[rcol] = m[rcol].copy(), m[lcol].copy()\n",
    "    return m\n",
    "\n",
    "def rotate_df(df, angle):\n",
    "    r = df.copy()\n",
    "    c, s = np.cos(angle), np.sin(angle)\n",
    "    for col in r.columns:\n",
    "        if col.endswith(\"_x\"):\n",
    "            base = col[:-2]\n",
    "            xcol, zcol = f\"{base}_x\", f\"{base}_z\"\n",
    "            if zcol in r:\n",
    "                x, z = df[xcol].values, df[zcol].values\n",
    "                r[xcol] = c*x - s*z\n",
    "                r[zcol] = s*x + c*z\n",
    "    return r\n",
    "\n",
    "aug_X = [X_train]\n",
    "aug_y = [y_train]\n",
    "aug_g = [groups_train]\n",
    "\n",
    "aug_X.append(mirror_df(X_train));       aug_y.append(y_train.copy()); aug_g.append(groups_train.copy())\n",
    "for ang in (np.deg2rad(15), np.deg2rad(-15)):\n",
    "    aug_X.append(rotate_df(X_train, ang)); aug_y.append(y_train.copy()); aug_g.append(groups_train.copy())\n",
    "\n",
    "X_train_aug       = pd.concat(aug_X, ignore_index=True)\n",
    "y_train_aug       = pd.concat(aug_y, ignore_index=True)\n",
    "groups_train_aug  = np.concatenate(aug_g)\n",
    "\n",
    "print(\"Train frames before aug:\", len(X_train))\n",
    "print(\"Train frames after  aug:\", len(X_train_aug))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a67820",
   "metadata": {},
   "source": [
    "### 3. SLIDING-WINDOWS \n",
    "\n",
    "**Purpose** :\n",
    "\n",
    "Convert raw frame-wise data into sequences for temporal modeling (e.g., LSTMs, CNNs).\n",
    "**Key Operations** :\n",
    " \n",
    "- Uses a fixed `window_size` and `step` to generate overlapping sequences.\n",
    " \n",
    "- Labels each sequence using majority voting (cut if more than half the frames are “cut”).\n",
    "\n",
    "**Significance** :\n",
    "\n",
    "Crucial for temporal deep learning models, turning static data into time series input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "609b73d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windowed data — train: (116312, 429), test: (6914, 429)\n"
     ]
    }
   ],
   "source": [
    "WINDOW = 11\n",
    "HALF   = WINDOW // 2\n",
    "\n",
    "def build_windows(X_df, y_arr, g_arr):\n",
    "    X_np = X_df.values\n",
    "    X_win, y_win, centre_global_idx = [], [], []\n",
    "    for i in range(HALF, len(X_np) - HALF):\n",
    "        if np.all(g_arr[i-HALF : i+HALF+1] == g_arr[i]):\n",
    "            X_win.append(X_np[i-HALF : i+HALF+1].ravel())\n",
    "            y_win.append(y_arr[i])\n",
    "            centre_global_idx.append(i)\n",
    "    return np.stack(X_win), np.array(y_win), np.array(centre_global_idx)\n",
    "\n",
    "X_win,      y_win,      centre_idx_train = build_windows(\n",
    "    X_train_aug, y_train_aug.values, groups_train_aug\n",
    ")\n",
    "X_test_win, y_test_win, centre_idx_test  = build_windows(\n",
    "    X_test, y_test.values, groups_test\n",
    ")\n",
    "\n",
    "print(f\"Windowed data — train: {X_win.shape}, test: {X_test_win.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e058a87c",
   "metadata": {},
   "source": [
    "### 4. SCALING & CLASS-WEIGHTS \n",
    "\n",
    "**Purpose** :\n",
    "\n",
    "Prepares data for training by normalizing input features and addressing class imbalance.\n",
    "**Key Operations** :\n",
    " \n",
    "- Applies `StandardScaler` fit on training data only.\n",
    " \n",
    "- Computes `class_weight` using `sklearn`’s `compute_class_weight`.\n",
    "\n",
    "**Significance** :\n",
    "\n",
    "Normalization improves training stability; class weighting prevents bias toward the majority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3cd4e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler  = StandardScaler().fit(X_win)\n",
    "X_tr    = scaler.transform(X_win)\n",
    "X_te    = scaler.transform(X_test_win)\n",
    "\n",
    "classes = np.unique(y_win)\n",
    "cw      = compute_class_weight(class_weight=\"balanced\",\n",
    "                               classes=classes,\n",
    "                               y=y_win)\n",
    "class_weight = dict(zip(classes, cw))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f134fc6",
   "metadata": {},
   "source": [
    "### 5. MODEL FACTORY (parametrized for grid search) \n",
    "\n",
    "**Purpose** :\n",
    "\n",
    "Defines a customizable model builder compatible with `GridSearchCV`.\n",
    "**Key Operations** :\n",
    " \n",
    "- Creates a sequential Keras model with a configurable number of `Conv1D` layers, dropout, and dense output.\n",
    " \n",
    "- Wraps the model using `KerasClassifier` from `scikeras`.\n",
    "\n",
    "**Significance** :\n",
    "\n",
    "Allows hyperparameter tuning of architecture components during grid search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "02598b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_layers=2, units=64, learning_rate=0.001):\n",
    "    m = models.Sequential()\n",
    "    m.add(layers.Input(shape=(X_tr.shape[1],)))\n",
    "    for _ in range(n_layers):\n",
    "        m.add(layers.Dense(units, activation=\"relu\"))\n",
    "    m.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    m.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\n",
    "            \"accuracy\",\n",
    "            tf.keras.metrics.Precision(name=\"precision\"),\n",
    "            tf.keras.metrics.Recall(name=\"recall\")\n",
    "        ]\n",
    "    )\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869dde0c",
   "metadata": {},
   "source": [
    "### 6. GRID SEARCH CV \n",
    "\n",
    "**Purpose** :\n",
    "\n",
    "Performs hyperparameter tuning with cross-validation to find the best model setup.\n",
    "**Key Operations** :\n",
    " \n",
    "- Defines a scoring metric using `f1_score`.\n",
    " \n",
    "- Uses `GroupShuffleSplit` with multiple splits for CV.\n",
    " \n",
    "- Grid searches over combinations of window size, dropout rate, learning rate, etc.\n",
    "\n",
    "**Significance** :\n",
    "\n",
    "Systematic tuning enhances performance and generalization by finding optimal configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9f4cba50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏳ Running GridSearchCV over 1 configs × 5-fold CV\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'super' object has no attribute '__sklearn_tags__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     22\u001b[39m grid = GridSearchCV(\n\u001b[32m     23\u001b[39m     estimator=reg,\n\u001b[32m     24\u001b[39m     param_grid=param_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     verbose=\u001b[32m3\u001b[39m\n\u001b[32m     30\u001b[39m )\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m⏳ Running GridSearchCV over \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.prod([\u001b[38;5;28mlen\u001b[39m(v)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mv\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mparam_grid.values()])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m configs × \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid.cv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-fold CV\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m grid_result = \u001b[43mgrid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_win\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🏆 Best hyper-parameters:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m grid_result.best_params_.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/py3.12.env/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/py3.12.env/lib/python3.12/site-packages/sklearn/model_selection/_search.py:933\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    929\u001b[39m params = _check_method_params(X, params=params)\n\u001b[32m    931\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._get_routed_params_for_fit(params)\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m cv_orig = check_cv(\u001b[38;5;28mself\u001b[39m.cv, y, classifier=\u001b[43mis_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    934\u001b[39m n_splits = cv_orig.get_n_splits(X, y, **routed_params.splitter.split)\n\u001b[32m    936\u001b[39m base_estimator = clone(\u001b[38;5;28mself\u001b[39m.estimator)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/py3.12.env/lib/python3.12/site-packages/sklearn/base.py:1237\u001b[39m, in \u001b[36mis_classifier\u001b[39m\u001b[34m(estimator)\u001b[39m\n\u001b[32m   1230\u001b[39m     warnings.warn(\n\u001b[32m   1231\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpassing a class to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mprint\u001b[39m(inspect.stack()[\u001b[32m0\u001b[39m][\u001b[32m3\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is deprecated and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1232\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in 1.8. Use an instance of the class instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1233\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m   1234\u001b[39m     )\n\u001b[32m   1235\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(estimator, \u001b[33m\"\u001b[39m\u001b[33m_estimator_type\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[33m\"\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m.estimator_type == \u001b[33m\"\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/py3.12.env/lib/python3.12/site-packages/sklearn/utils/_tags.py:430\u001b[39m, in \u001b[36mget_tags\u001b[39m\u001b[34m(estimator)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m klass \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mtype\u001b[39m(estimator).mro()):\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m__sklearn_tags__\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m         sklearn_tags_provider[klass] = \u001b[43mklass\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__sklearn_tags__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    431\u001b[39m         class_order.append(klass)\n\u001b[32m    432\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_more_tags\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.10/envs/py3.12.env/lib/python3.12/site-packages/sklearn/base.py:540\u001b[39m, in \u001b[36mClassifierMixin.__sklearn_tags__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__sklearn_tags__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m     tags = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__sklearn_tags__\u001b[49m()\n\u001b[32m    541\u001b[39m     tags.estimator_type = \u001b[33m\"\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    542\u001b[39m     tags.classifier_tags = ClassifierTags()\n",
      "\u001b[31mAttributeError\u001b[39m: 'super' object has no attribute '__sklearn_tags__'"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"model__n_layers\":          [12],              # medium vs. slightly deeper net\n",
    "    \"model__units\":             [128],           # moderate vs. higher capacity\n",
    "    \"optimizer__learning_rate\": [1e-4],        # standard Adam LR vs. a smaller step\n",
    "    \"batch_size\":               [128],           # small vs. medium batches\n",
    "    \"epochs\":                   [50],           # enough to converge vs. extra training\n",
    "}\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "reg = KerasClassifier(\n",
    "    model=build_model,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=reg,\n",
    "    param_grid=param_grid,\n",
    "    scoring=f1_scorer,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    verbose=3\n",
    ")\n",
    "\n",
    "print(f\"\\n⏳ Running GridSearchCV over {np.prod([len(v) for v in param_grid.values()])} configs × {grid.cv}-fold CV\\n\")\n",
    "grid_result = grid.fit(X_tr, y_win)\n",
    "\n",
    "print(\"\\n🏆 Best hyper-parameters:\")\n",
    "for k, v in grid_result.best_params_.items():\n",
    "    print(f\"   • {k:20s}: {v}\")\n",
    "print(\"Best CV F1-score :\", grid_result.best_score_)\n",
    "\n",
    "best_model = grid_result.best_estimator_.model_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0ab824",
   "metadata": {},
   "source": [
    "### 7. EVALUATION (use best_model) \n",
    "\n",
    "**Purpose** :\n",
    "\n",
    "Applies the best model to the test set and reports performance metrics.\n",
    "**Key Operations** :\n",
    " \n",
    "- Uses the `best_estimator_` from the grid search.\n",
    " \n",
    "- Evaluates precision, recall, and F1 score on the test set.\n",
    " \n",
    "- Optionally plots a confusion matrix or prints a classification report.\n",
    "\n",
    "**Significance** :\n",
    "\n",
    "Quantifies the effectiveness of the final model on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f103b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc, prec, rec = best_model.evaluate(X_te, y_test_win, verbose=0)\n",
    "f1 = 2 * (prec * rec) / (prec + rec + 1e-8)\n",
    "print(f\"\\nWindowed-test → loss {loss:.4f}  acc {acc:.4f}  precision {prec:.4f}  recall {rec:.4f}  F1 {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923872f6",
   "metadata": {},
   "source": [
    "### 8. BOUNDARY-ERROR EVALUATION \n",
    "\n",
    "**Purpose** :\n",
    "\n",
    "Special evaluation metric for segmentation tasks—detects temporal localization errors.\n",
    "**Key Operations** :\n",
    " \n",
    "- Compares predicted vs. actual “cut” boundaries.\n",
    " \n",
    "- Allows small temporal error margin (`delta`).\n",
    " \n",
    "- Measures over/under-segmentation using frame offsets.\n",
    "\n",
    "**Significance** :\n",
    "\n",
    "Goes beyond frame accuracy, measuring how well the model detects transitions (important for action segmentation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36f80a1",
   "metadata": {},
   "source": [
    "### 9. SAVE ARTIFACTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd756574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model.save(\"kinect_cutting_model.keras\")\n",
    "\n",
    "# Save the input scaler\n",
    "# dump(scaler, \"kinect_cutting_scaler.pkl\")\n",
    "\n",
    "# print(\"\\n💾  Model saved to kinect_cutting_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a1d2ae",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Result\n",
    "\n",
    "───────── Aggregate boundary error ─────────\n",
    "\n",
    "Δstart  mean  +2.83 ± 19.14   median |Δ| 5.0 frames\n",
    "\n",
    "Δend    mean  +1.22 ± 16.01   median |Δ| 3.0 frames\n",
    "\n",
    "### These results indicate:\n",
    "\n",
    "Start frames are predicted, on average, 2.83 frames later than actual.\n",
    "End frames are predicted, on average, 1.22 frames later than actual.\n",
    "The median absolute errors (5.0 for start, 3.0 for end) suggest relatively good accuracy in boundary detection.\n",
    "Standard deviations (19.14 for start, 16.01 for end) indicate some variability in predictions.\n",
    "Overall, the model shows promising performance in detecting exercise segment boundaries, with slightly better accuracy for end frames compared to start frames.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7adf514",
   "metadata": {},
   "source": [
    "## Software development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94181bf",
   "metadata": {},
   "source": [
    "# Frame Trimmer Implementation Progress Report\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "We have successfully implemented the Frame Trimmer component for the ML Prediction Dashboard that allows users to:\n",
    "\n",
    "1. Upload CSV files containing skeletal keypoint data\n",
    "2. Send the CSV to the backend for frame trimming\n",
    "3. Visualize both the original and trimmed data in a side-by-side 3D comparison\n",
    "4. Clearly see which frames were kept vs. removed during the trimming process\n",
    "\n",
    "The Frame Trimmer has been integrated as a new tab in the existing dashboard alongside the \"Predictions\" and \"PoseNet\" tabs.\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "### Frontend Components\n",
    "\n",
    "1. **FrameTrimmer.jsx**\n",
    "   - Main component that handles file upload, processing, and visualization\n",
    "   - Integrated with the existing SkeletonContext and SkeletonRenderer components\n",
    "   - Includes a visual timeline showing which frames were kept vs. removed\n",
    "   - Implements custom playback controls that handle all frames\n",
    "\n",
    "2. **Custom Animation and Controls**\n",
    "   - Replaced the standard AnimationManager with a custom implementation\n",
    "   - Created custom controls to properly handle the original frame count\n",
    "   - Fixed issues with playback that previously stopped at the end of trimmed data\n",
    "\n",
    "3. **Visual Frame Timeline**\n",
    "   - Implemented a series of colored div elements to create a timeline\n",
    "   - Green segments show frames that were kept in the trimmed dataset\n",
    "   - Red segments show frames that were removed\n",
    "   - Interactive elements allow users to click to jump to specific frames\n",
    "   - Optimized for performance with large datasets by implementing frame sampling\n",
    "\n",
    "### Backend Endpoint\n",
    "\n",
    "1. **`/trim-frames` Endpoint**\n",
    "   - Added a new POST endpoint to the FastAPI application\n",
    "   - Accepts a CSV file and returns a trimmed version\n",
    "   - Currently returns a pre-trimmed output.csv file using our model(manually, without the pipeline) for testing\n",
    "   - Returns the CSV content directly as text\n",
    "\n",
    "## Technical Challenges and Interim Solutions\n",
    "\n",
    "### Backend Processing Issues\n",
    "\n",
    "1. **Temporary Manual Workflow**\n",
    "   - **IMPORTANT NOTE:** Due to time constraints and the need to deliver a functional demonstration, we've established a temporary manual workflow:\n",
    "     1. Manually preprocess CSV files using the working script\n",
    "     2. Upload the processed files to the test folder of the backend\n",
    "     3. Compare the uploaded uncut data with the manually prepared trimmed data(uncut.csv can also be found in the backend folder)\n",
    "   - This approach allows us to showcase the functionality while we address the backend processing issues\n",
    "\n",
    "\n",
    "\n",
    "## User Experience Improvements\n",
    "\n",
    "1. **Intuitive Visualization**\n",
    "   - Color-coded timeline provides immediate visual feedback on trimmed frames\n",
    "   - Clear indicators show which frames were kept vs. removed\n",
    "   - Empty placeholder for removed frames clearly communicates when a frame was trimmed\n",
    "\n",
    "2. **Feedback and Status**\n",
    "   - Added informative badges showing frame counts and percentage removed\n",
    "   - Included status indicators to show current frame in both datasets\n",
    "   - Added an informative badge explaining the color coding\n",
    "\n",
    "3. **Responsive Controls**\n",
    "   - Implemented smooth playback controls\n",
    "   - Added interactive timeline with click-to-seek functionality\n",
    "   - Ensured responsive design for different screen sizes\n",
    "\n",
    "## Technical Architecture\n",
    "\n",
    "The Frame Trimmer follows a clean architecture pattern:\n",
    "\n",
    "1. **Data Flow**\n",
    "   - User uploads CSV file → Frontend parses and displays original data\n",
    "   - CSV is sent to backend → Backend processes and returns trimmed data\n",
    "   - Frontend parses trimmed data → Both datasets visualized side by side\n",
    "\n",
    "2. **State Management**\n",
    "   - Uses React useState for component-level state\n",
    "   - Leverages SkeletonContext for shared visualization state\n",
    "   - Maintains mapping between original and trimmed frames\n",
    "\n",
    "3. **Optimization Techniques**\n",
    "   - Memoization of expensive calculations with useMemo\n",
    "   - Callback optimization with useCallback\n",
    "   - Conditional rendering to reduce DOM elements\n",
    "   - Sampling for large datasets to maintain performance\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Despite the backend processing challenges, the Frame Trimmer component successfully integrates with the existing ML Prediction Dashboard, providing a powerful tool for visualizing frame trimming results. The current manual workflow is a temporary solution that allows us to demonstrate the functionality while we work on fixing the backend issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

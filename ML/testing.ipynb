{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kinect XY to Z Coordinate Prediction with TensorFlow\n",
    "Introduction\n",
    "In this notebook, we will build a deep learning model to predict the Z coordinates of Kinect body joints from their X and Y coordinates. The Kinect sensor provides 3D joint positions (X, Y, Z) for 13 joints across a sequence of frames. Our goal is to train a model that uses the X and Y values as input to estimate the corresponding Z value for each joint. To make use of temporal information, we will incorporate a sliding window of frames (default window size = 5) as the model input. This means the model will consider a short sequence of consecutive frames when making each prediction, providing context that can improve accuracy. We will experiment with different neural network architectures (fully-connected Dense layers, 1D convolutional layers, and a hybrid of both) to see which works best for this task. Plan:\n",
    "Load and inspect the dataset: Ensure it has 13 joints with X, Y, Z coordinates per frame.\n",
    "Prepare the data: Separate input features (X and Y for all joints) and target outputs (Z for all joints). Create sequence data using a sliding window of frames, and optionally normalize the features for better training.\n",
    "Define a modular model architecture: Build a function to create a TensorFlow model that can be configured for three modes: Dense-only, Conv1D-only, or a hybrid (Conv1D + Dense). We will choose appropriate activation functions (ReLU for hidden layers, linear for output) and weight initializations (He initialization for ReLU layers) for each.\n",
    "Compile the model: Use a stochastic gradient descent optimizer (e.g. Adam, an adaptive SGD variant) with a regression loss (Mean Squared Error) and track Mean Absolute Error (MAE) as an additional metric.\n",
    "Train and evaluate with 10-fold cross-validation: Split the data into 10 folds, train the model on 9 folds and validate on the 1 remaining fold, rotating through all folds. This provides a robust evaluation of model performance on all data.\n",
    "Use callbacks for early stopping and checkpointing: Integrate an EarlyStopping callback to halt training when validation loss stops improving (preventing overfitting and saving time) and a ModelCheckpoint callback to save the best model weights to disk. We will explain how to adjust or disable these features as needed.\n",
    "Analyze results: For each fold/architecture, we'll record the performance (MSE and MAE) and compute the average across folds. This will help compare the architectures and ensure the model generalizes well.\n",
    "Let's get started by importing necessary libraries and loading the dataset.\n",
    "Import Libraries and Load Data\n",
    "First, we import the required libraries. We will use pandas for data loading/manipulation, NumPy for numerical operations, scikit-learn for data splitting and normalization, and TensorFlow/Keras for building the neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load the Kinect joint data. We assume the data is stored in one or multiple CSV files where each row corresponds to a frame and contains the 3D coordinates for 13 joints (x, y, z for each joint). If the data is split across multiple files (e.g., different recordings or subjects), we will load and combine them. Each file is expected to have columns like head_x, head_y, head_z, left_shoulder_x, ... right_foot_z (13 joints × 3 coordinates = 39 columns). There may also be an index or frame number column which we will drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded A100_kinect.csv\n",
      "Successfully loaded A101_kinect.csv\n",
      "Successfully loaded A102_kinect.csv\n",
      "Successfully loaded A103_kinect.csv\n",
      "Successfully loaded A104_kinect.csv\n",
      "Successfully loaded A105_kinect.csv\n",
      "Successfully loaded A106_kinect.csv\n",
      "Successfully loaded A108_kinect.csv\n",
      "Successfully loaded A109_kinect.csv\n",
      "Successfully loaded A10_kinect.csv\n",
      "Successfully loaded A110_kinect.csv\n",
      "Successfully loaded A111_kinect.csv\n",
      "Successfully loaded A112_kinect.csv\n",
      "Successfully loaded A113_kinect.csv\n",
      "Successfully loaded A114_kinect.csv\n",
      "Successfully loaded A115_kinect.csv\n",
      "Successfully loaded A116_kinect.csv\n",
      "Successfully loaded A117_kinect.csv\n",
      "Successfully loaded A118_kinect.csv\n",
      "Successfully loaded A119_kinect.csv\n",
      "Successfully loaded A11_kinect.csv\n",
      "Successfully loaded A120_kinect.csv\n",
      "Successfully loaded A121_kinect.csv\n",
      "Successfully loaded A122_kinect.csv\n",
      "Successfully loaded A123_kinect.csv\n",
      "Successfully loaded A124_kinect.csv\n",
      "Successfully loaded A125_kinect.csv\n",
      "Successfully loaded A126_kinect.csv\n",
      "Successfully loaded A127_kinect.csv\n",
      "Successfully loaded A128_kinect.csv\n",
      "Successfully loaded A129_kinect.csv\n",
      "Successfully loaded A12_kinect.csv\n",
      "Successfully loaded A130_kinect.csv\n",
      "Successfully loaded A131_kinect.csv\n",
      "Successfully loaded A132_kinect.csv\n",
      "Successfully loaded A133_kinect.csv\n",
      "Successfully loaded A134_kinect.csv\n",
      "Successfully loaded A135_kinect.csv\n",
      "Successfully loaded A136_kinect.csv\n",
      "Successfully loaded A137_kinect.csv\n",
      "Successfully loaded A138_kinect.csv\n",
      "Successfully loaded A139_kinect.csv\n",
      "Successfully loaded A13_kinect.csv\n",
      "Successfully loaded A140_kinect.csv\n",
      "Successfully loaded A141_kinect.csv\n",
      "Successfully loaded A142_kinect.csv\n",
      "Successfully loaded A143_kinect.csv\n",
      "Successfully loaded A144_kinect.csv\n",
      "Successfully loaded A145_kinect.csv\n",
      "Successfully loaded A146_kinect.csv\n",
      "Successfully loaded A147_kinect.csv\n",
      "Successfully loaded A148_kinect.csv\n",
      "Successfully loaded A149_kinect.csv\n",
      "Successfully loaded A14_kinect.csv\n",
      "Successfully loaded A150_kinect.csv\n",
      "Successfully loaded A151_kinect.csv\n",
      "Successfully loaded A152_kinect.csv\n",
      "Successfully loaded A153_kinect.csv\n",
      "Successfully loaded A154_kinect.csv\n",
      "Successfully loaded A155_kinect.csv\n",
      "Successfully loaded A156_kinect.csv\n",
      "Successfully loaded A157_kinect.csv\n",
      "Successfully loaded A158_kinect.csv\n",
      "Successfully loaded A159_kinect.csv\n",
      "Successfully loaded A15_kinect.csv\n",
      "Successfully loaded A16_kinect.csv\n",
      "Successfully loaded A17_kinect.csv\n",
      "Successfully loaded A18_kinect.csv\n",
      "Successfully loaded A19_kinect.csv\n",
      "Successfully loaded A1_kinect.csv\n",
      "Successfully loaded A20_kinect.csv\n",
      "Successfully loaded A21_kinect.csv\n",
      "Successfully loaded A22_kinect.csv\n",
      "Successfully loaded A23_kinect.csv\n",
      "Successfully loaded A24_kinect.csv\n",
      "Successfully loaded A25_kinect.csv\n",
      "Successfully loaded A26_kinect.csv\n",
      "Successfully loaded A27_kinect.csv\n",
      "Successfully loaded A28_kinect.csv\n",
      "Successfully loaded A29_kinect.csv\n",
      "Successfully loaded A2_kinect.csv\n",
      "Successfully loaded A30_kinect.csv\n",
      "Successfully loaded A31_kinect.csv\n",
      "Successfully loaded A32_kinect.csv\n",
      "Successfully loaded A33_kinect.csv\n",
      "Successfully loaded A34_kinect.csv\n",
      "Successfully loaded A35_kinect.csv\n",
      "Successfully loaded A36_kinect.csv\n",
      "Successfully loaded A37_kinect.csv\n",
      "Successfully loaded A38_kinect.csv\n",
      "Successfully loaded A39_kinect.csv\n",
      "Successfully loaded A3_kinect.csv\n",
      "Successfully loaded A40_kinect.csv\n",
      "Successfully loaded A41_kinect.csv\n",
      "Successfully loaded A42_kinect.csv\n",
      "Successfully loaded A43_kinect.csv\n",
      "Successfully loaded A44_kinect.csv\n",
      "Successfully loaded A45_kinect.csv\n",
      "Successfully loaded A46_kinect.csv\n",
      "Successfully loaded A47_kinect.csv\n",
      "Successfully loaded A48_kinect.csv\n",
      "Successfully loaded A49_kinect.csv\n",
      "Successfully loaded A4_kinect.csv\n",
      "Successfully loaded A50_kinect.csv\n",
      "Successfully loaded A51_kinect.csv\n",
      "Successfully loaded A52_kinect.csv\n",
      "Successfully loaded A53_kinect.csv\n",
      "Successfully loaded A54_kinect.csv\n",
      "Successfully loaded A55_kinect.csv\n",
      "Successfully loaded A56_kinect.csv\n",
      "Successfully loaded A57_kinect.csv\n",
      "Successfully loaded A58_kinect.csv\n",
      "Successfully loaded A59_kinect.csv\n",
      "Successfully loaded A5_kinect.csv\n",
      "Successfully loaded A61_kinect.csv\n",
      "Successfully loaded A62_kinect.csv\n",
      "Successfully loaded A63_kinect.csv\n",
      "Successfully loaded A64_kinect.csv\n",
      "Successfully loaded A65_kinect.csv\n",
      "Successfully loaded A66_kinect.csv\n",
      "Successfully loaded A67_kinect.csv\n",
      "Successfully loaded A68_kinect.csv\n",
      "Successfully loaded A69_kinect.csv\n",
      "Successfully loaded A6_kinect.csv\n",
      "Successfully loaded A70_kinect.csv\n",
      "Successfully loaded A71_kinect.csv\n",
      "Successfully loaded A72_kinect.csv\n",
      "Successfully loaded A73_kinect.csv\n",
      "Successfully loaded A74_kinect.csv\n",
      "Successfully loaded A75_kinect.csv\n",
      "Successfully loaded A76_kinect.csv\n",
      "Successfully loaded A77_kinect.csv\n",
      "Successfully loaded A78_kinect.csv\n",
      "Successfully loaded A79_kinect.csv\n",
      "Successfully loaded A7_kinect.csv\n",
      "Successfully loaded A80_kinect.csv\n",
      "Successfully loaded A81_kinect.csv\n",
      "Successfully loaded A82_kinect.csv\n",
      "Successfully loaded A83_kinect.csv\n",
      "Successfully loaded A84_kinect.csv\n",
      "Successfully loaded A85_kinect.csv\n",
      "Successfully loaded A86_kinect.csv\n",
      "Successfully loaded A87_kinect.csv\n",
      "Successfully loaded A88_kinect.csv\n",
      "Successfully loaded A89_kinect.csv\n",
      "Successfully loaded A8_kinect.csv\n",
      "Successfully loaded A90_kinect.csv\n",
      "Successfully loaded A91_kinect.csv\n",
      "Successfully loaded A92_kinect.csv\n",
      "Successfully loaded A93_kinect.csv\n",
      "Successfully loaded A94_kinect.csv\n",
      "Successfully loaded A95_kinect.csv\n",
      "Successfully loaded A96_kinect.csv\n",
      "Successfully loaded A97_kinect.csv\n",
      "Successfully loaded A98_kinect.csv\n",
      "Successfully loaded A99_kinect.csv\n",
      "Successfully loaded A9_kinect.csv\n",
      "Successfully loaded B10_kinect.csv\n",
      "Successfully loaded B11_kinect.csv\n",
      "Successfully loaded B12_kinect.csv\n",
      "Successfully loaded B13_kinect.csv\n",
      "Successfully loaded B14_kinect.csv\n",
      "Successfully loaded B15_kinect.csv\n",
      "Successfully loaded B16_kinect.csv\n",
      "Successfully loaded B17_kinect.csv\n",
      "Successfully loaded B18_kinect.csv\n",
      "Successfully loaded B19_kinect.csv\n",
      "Successfully loaded B1_kinect.csv\n",
      "Successfully loaded B20_kinect.csv\n",
      "Successfully loaded B21_kinect.csv\n",
      "Successfully loaded B22_kinect.csv\n",
      "Successfully loaded B2_kinect.csv\n",
      "Successfully loaded B3_kinect.csv\n",
      "Successfully loaded B4_kinect.csv\n",
      "Successfully loaded B5_kinect.csv\n",
      "Successfully loaded B6_kinect.csv\n",
      "Successfully loaded B7_kinect.csv\n",
      "Successfully loaded B8_kinect.csv\n",
      "Successfully loaded B9_kinect.csv\n",
      "Dataframe shape after dropping FrameNo: (24005, 39)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head_x</th>\n",
       "      <th>head_y</th>\n",
       "      <th>head_z</th>\n",
       "      <th>left_shoulder_x</th>\n",
       "      <th>left_shoulder_y</th>\n",
       "      <th>left_shoulder_z</th>\n",
       "      <th>left_elbow_x</th>\n",
       "      <th>left_elbow_y</th>\n",
       "      <th>left_elbow_z</th>\n",
       "      <th>right_shoulder_x</th>\n",
       "      <th>...</th>\n",
       "      <th>left_knee_z</th>\n",
       "      <th>right_knee_x</th>\n",
       "      <th>right_knee_y</th>\n",
       "      <th>right_knee_z</th>\n",
       "      <th>left_foot_x</th>\n",
       "      <th>left_foot_y</th>\n",
       "      <th>left_foot_z</th>\n",
       "      <th>right_foot_x</th>\n",
       "      <th>right_foot_y</th>\n",
       "      <th>right_foot_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.016480</td>\n",
       "      <td>0.77960</td>\n",
       "      <td>0.070646</td>\n",
       "      <td>-0.16221</td>\n",
       "      <td>0.58030</td>\n",
       "      <td>0.047752</td>\n",
       "      <td>-0.23056</td>\n",
       "      <td>0.84519</td>\n",
       "      <td>-0.033909</td>\n",
       "      <td>0.17278</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046769</td>\n",
       "      <td>0.13624</td>\n",
       "      <td>-0.51948</td>\n",
       "      <td>0.003528</td>\n",
       "      <td>-0.13975</td>\n",
       "      <td>-0.86193</td>\n",
       "      <td>-0.025068</td>\n",
       "      <td>0.15162</td>\n",
       "      <td>-0.91701</td>\n",
       "      <td>-0.005627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.016990</td>\n",
       "      <td>0.77937</td>\n",
       "      <td>0.069989</td>\n",
       "      <td>-0.16242</td>\n",
       "      <td>0.58031</td>\n",
       "      <td>0.047168</td>\n",
       "      <td>-0.23050</td>\n",
       "      <td>0.84547</td>\n",
       "      <td>-0.034061</td>\n",
       "      <td>0.17334</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045948</td>\n",
       "      <td>0.13663</td>\n",
       "      <td>-0.51921</td>\n",
       "      <td>0.003291</td>\n",
       "      <td>-0.13982</td>\n",
       "      <td>-0.86208</td>\n",
       "      <td>-0.024980</td>\n",
       "      <td>0.15001</td>\n",
       "      <td>-0.91352</td>\n",
       "      <td>-0.001491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.017345</td>\n",
       "      <td>0.77928</td>\n",
       "      <td>0.069703</td>\n",
       "      <td>-0.16258</td>\n",
       "      <td>0.58039</td>\n",
       "      <td>0.046964</td>\n",
       "      <td>-0.23044</td>\n",
       "      <td>0.84564</td>\n",
       "      <td>-0.034175</td>\n",
       "      <td>0.17310</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039191</td>\n",
       "      <td>0.13633</td>\n",
       "      <td>-0.51957</td>\n",
       "      <td>0.003223</td>\n",
       "      <td>-0.13937</td>\n",
       "      <td>-0.86233</td>\n",
       "      <td>-0.024777</td>\n",
       "      <td>0.14981</td>\n",
       "      <td>-0.91385</td>\n",
       "      <td>-0.001252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.017602</td>\n",
       "      <td>0.77935</td>\n",
       "      <td>0.069520</td>\n",
       "      <td>-0.16276</td>\n",
       "      <td>0.58035</td>\n",
       "      <td>0.046524</td>\n",
       "      <td>-0.23037</td>\n",
       "      <td>0.84598</td>\n",
       "      <td>-0.034318</td>\n",
       "      <td>0.17349</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044046</td>\n",
       "      <td>0.13653</td>\n",
       "      <td>-0.51892</td>\n",
       "      <td>0.002961</td>\n",
       "      <td>-0.13907</td>\n",
       "      <td>-0.86225</td>\n",
       "      <td>-0.024431</td>\n",
       "      <td>0.15711</td>\n",
       "      <td>-0.92048</td>\n",
       "      <td>-0.014920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.017559</td>\n",
       "      <td>0.77938</td>\n",
       "      <td>0.069530</td>\n",
       "      <td>-0.16279</td>\n",
       "      <td>0.58030</td>\n",
       "      <td>0.046288</td>\n",
       "      <td>-0.22906</td>\n",
       "      <td>0.85070</td>\n",
       "      <td>-0.036518</td>\n",
       "      <td>0.17365</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047814</td>\n",
       "      <td>0.13662</td>\n",
       "      <td>-0.51940</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>-0.13917</td>\n",
       "      <td>-0.86364</td>\n",
       "      <td>-0.025274</td>\n",
       "      <td>0.15212</td>\n",
       "      <td>-0.91871</td>\n",
       "      <td>-0.004539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     head_x   head_y    head_z  left_shoulder_x  left_shoulder_y  \\\n",
       "0 -0.016480  0.77960  0.070646         -0.16221          0.58030   \n",
       "1 -0.016990  0.77937  0.069989         -0.16242          0.58031   \n",
       "2 -0.017345  0.77928  0.069703         -0.16258          0.58039   \n",
       "3 -0.017602  0.77935  0.069520         -0.16276          0.58035   \n",
       "4 -0.017559  0.77938  0.069530         -0.16279          0.58030   \n",
       "\n",
       "   left_shoulder_z  left_elbow_x  left_elbow_y  left_elbow_z  \\\n",
       "0         0.047752      -0.23056       0.84519     -0.033909   \n",
       "1         0.047168      -0.23050       0.84547     -0.034061   \n",
       "2         0.046964      -0.23044       0.84564     -0.034175   \n",
       "3         0.046524      -0.23037       0.84598     -0.034318   \n",
       "4         0.046288      -0.22906       0.85070     -0.036518   \n",
       "\n",
       "   right_shoulder_x  ...  left_knee_z  right_knee_x  right_knee_y  \\\n",
       "0           0.17278  ...    -0.046769       0.13624      -0.51948   \n",
       "1           0.17334  ...    -0.045948       0.13663      -0.51921   \n",
       "2           0.17310  ...    -0.039191       0.13633      -0.51957   \n",
       "3           0.17349  ...    -0.044046       0.13653      -0.51892   \n",
       "4           0.17365  ...    -0.047814       0.13662      -0.51940   \n",
       "\n",
       "   right_knee_z  left_foot_x  left_foot_y  left_foot_z  right_foot_x  \\\n",
       "0      0.003528     -0.13975     -0.86193    -0.025068       0.15162   \n",
       "1      0.003291     -0.13982     -0.86208    -0.024980       0.15001   \n",
       "2      0.003223     -0.13937     -0.86233    -0.024777       0.14981   \n",
       "3      0.002961     -0.13907     -0.86225    -0.024431       0.15711   \n",
       "4      0.002998     -0.13917     -0.86364    -0.025274       0.15212   \n",
       "\n",
       "   right_foot_y  right_foot_z  \n",
       "0      -0.91701     -0.005627  \n",
       "1      -0.91352     -0.001491  \n",
       "2      -0.91385     -0.001252  \n",
       "3      -0.92048     -0.014920  \n",
       "4      -0.91871     -0.004539  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = 'data/kinect_good_preprocessed'\n",
    "\n",
    "# List all CSV files that end with kinect.csv\n",
    "csv_files = [f for f in os.listdir(data_dir) if f.endswith('kinect.csv')]\n",
    "\n",
    "# Create an empty dataframe\n",
    "data = pd.DataFrame()\n",
    "\n",
    "# Load each CSV file and concatenate to the main dataframe\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Add a source column to track which file just to check\n",
    "        # df['source_file'] = file\n",
    "\n",
    "        # Append to the main dataframe\n",
    "        data = pd.concat([data, df], ignore_index=True)\n",
    "\n",
    "        print(f\"Successfully loaded {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "# Drop the FrameNo column if it exists\n",
    "if 'FrameNo' in data.columns:\n",
    "    data = data.drop('FrameNo', axis=1)\n",
    "\n",
    "# Check the dataframe after dropping FrameNo\n",
    "print(f\"Dataframe shape after dropping FrameNo: {data.shape}\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: The code above reads all CSV files containing Kinect data and concatenates them into one pandas DataFrame called data. If there's a FrameNo column (which just indexes frames), we drop it since we don't need it for learning. We then print the shape and a subset of column names to verify the data loaded correctly. The expected shape should be (num_frames, 39) because we have 39 features (13 joints × 3 coordinates) per frame.\n",
    "Data Preprocessing\n",
    "Separating Features (X, Y) and Targets (Z)\n",
    "We need to use the X and Y coordinates as input features and the Z coordinates as the target output. Let's split the DataFrame into X (containing all _x and _y columns) and y (containing all _z columns). We will also convert them to NumPy arrays for TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape (X): (24005, 26)\n",
      "Targets shape (y): (24005, 13)\n"
     ]
    }
   ],
   "source": [
    "# Separate input features (XY coordinates) and target (Z coordinates)\n",
    "# Identify columns by suffix\n",
    "x_columns = [col for col in data.columns if col.endswith('_x')]\n",
    "y_columns = [col for col in data.columns if col.endswith('_y')]\n",
    "z_columns = [col for col in data.columns if col.endswith('_z')]\n",
    "\n",
    "# Sort columns to maintain consistent order (optional, for safety)\n",
    "x_columns = sorted(x_columns)\n",
    "y_columns = sorted(y_columns)\n",
    "z_columns = sorted(z_columns)\n",
    "\n",
    "# Prepare feature matrix X and target matrix y\n",
    "X = data[x_columns + y_columns].values  # shape: (num_frames, 26) -> 13 joints * 2 (x,y)\n",
    "y = data[z_columns].values             # shape: (num_frames, 13) -> 13 joints * 1 (z)\n",
    "\n",
    "print(\"Features shape (X):\", X.shape)\n",
    "print(\"Targets shape (y):\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: We filter column names by their suffix (_x, _y, _z) to gather the respective coordinate sets. There should be 13 columns for each suffix (13 X's, 13 Y's, 13 Z's). We then form X by concatenating the X and Y columns, resulting in 26 features per frame, and y from the Z columns, resulting in 13 target values per frame. We print the shapes to verify; for example, you might see Features shape: (24005, 26) and Targets shape: (24005, 13) if there are 24,005 frames in total.\n",
    "Creating Sequence Data with a Sliding Window\n",
    "Instead of predicting Z from a single frame's X and Y, we will use a window of consecutive frames as input. This means each training sample will consist of window_size frames of X/Y data, and the target will be the Z values of the last frame in that sequence. By default, we'll use window_size = 5, but this can be adjusted. For example, if window_size=5, then frames 1–5 (their X and Y values) will be used to predict frame 5's Z values; frames 2–6 will predict frame 6's Z, and so on. This provides the model with context from the preceding 4 frames when predicting the 5th frame's depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence input shape (X_seq): (23996, 10, 26)\n",
      "Sequence target shape (y_seq): (23996, 13)\n"
     ]
    }
   ],
   "source": [
    "# Create sequences of frames for input using a sliding window\n",
    "window_size = 10  # you can change this to use more or fewer frames in each input sequence\n",
    "\n",
    "X_seq = []\n",
    "y_seq = []\n",
    "num_frames = X.shape[0]\n",
    "for i in range(num_frames - window_size + 1):\n",
    "    # Take window_size consecutive frames for input\n",
    "    X_seq.append(X[i : i+window_size])\n",
    "    # Take the Z targets of the last frame in the sequence as the output\n",
    "    y_seq.append(y[i+window_size - 1])\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "print(\"Sequence input shape (X_seq):\", X_seq.shape)\n",
    "print(\"Sequence target shape (y_seq):\", y_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: We iterate through the data to build sequences of length window_size. X_seq will have shape (N_sequences, window_size, 26) where 26 is the number of input features per frame (X and Y for 13 joints). y_seq will have shape (N_sequences, 13) corresponding to the Z values for the last frame of each sequence. If our original data had N frames, the number of sequences N_sequences will be N - window_size + 1 (because we can't start a sequence in the very last frames if they don't have window_size frames ahead of them). For instance, with 24,005 frames and window_size=5, X_seq would have 24,001 sequences (since 4 frames are used to start the first sequence).\n",
    "Feature Scaling (Normalization)\n",
    "Neural networks often train faster and more reliably when input features are on similar scales. Here, X and Y coordinates might already be in a similar range (e.g., meters in a Kinect coordinate system), but it's still beneficial to normalize. We will use StandardScaler to standardize features: each feature (coordinate) will be scaled to have mean 0 and standard deviation 1 based on the training data. We will also scale the target Z values similarly. Important: When doing cross-validation, we must be careful to fit the scaler on the training portion of each fold and apply it to that fold’s validation data, to avoid leaking information. We'll handle this inside the cross-validation loop. (If you prefer not to scale the outputs because they are already in a known range, you can skip scaling y. But scaling X is highly recommended.)\n",
    "Building a Modular Neural Network Model\n",
    "Next, we'll define a function to create our TensorFlow model. This function will allow us to switch between different architectures:\n",
    "Dense-only: A fully-connected network that treats each sequence as a flattened vector (no explicit temporal processing, aside from what the window provides inherently).\n",
    "Conv1D-only: A model that uses 1D convolution layers over the time dimension to automatically learn temporal features from the sequence.\n",
    "Hybrid (Conv + Dense): A combination where we first apply a convolutional layer to extract temporal features, then flatten and pass through dense layers for further processing.\n",
    "Each architecture will share the same input-output structure: input shape corresponds to our sequence of frames, and output is 13 numbers (predicted Z for each joint). We will use ReLU activation for hidden layers (since it's a good default for many problems) and linear activation for the output (because this is a regression task and we want to predict continuous values without bounding them). We will also use He initialization (he_normal) for layers with ReLU, which is known to help train deep networks with ReLU activations by initializing weights in a suitable range​\n",
    "file-7q2hg52vvsvnwofjxdtetf\n",
    "​\n",
    "file-7q2hg52vvsvnwofjxdtetf\n",
    ". Let's define the model-building function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_34\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_34\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_69 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_70 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">845</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_34 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_69 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m33,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_70 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_71 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)             │           \u001b[38;5;34m845\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">42,509</span> (166.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m42,509\u001b[0m (166.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">42,509</span> (166.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m42,509\u001b[0m (166.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Conv model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_35\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_35\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_72 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,333</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_35 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m26\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_33 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m5,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_34 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m12,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_22 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m640\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_72 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)             │         \u001b[38;5;34m8,333\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,741</span> (100.55 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,741\u001b[0m (100.55 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,741</span> (100.55 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,741\u001b[0m (100.55 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hybrid model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_36\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_36\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_73 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">845</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_36 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m26\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_35 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m5,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_23 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m640\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_73 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m41,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_74 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)             │           \u001b[38;5;34m845\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,925</span> (183.30 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m46,925\u001b[0m (183.30 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,925</span> (183.30 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m46,925\u001b[0m (183.30 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_model(model_type='dense', window_size=5, n_features=26, n_outputs=13):\n",
    "    \"\"\"\n",
    "    Create a TensorFlow Keras model for predicting joint Z coordinates from XY coordinates.\n",
    "    model_type: 'dense', 'conv', or 'hybrid' to specify the architecture.\n",
    "    window_size: number of frames in the input sequence.\n",
    "    n_features: number of features per frame (should be 26 for 13 joints' X and Y).\n",
    "    n_outputs: number of outputs (13 joints' Z values).\n",
    "    \"\"\"\n",
    "    if model_type == 'dense':\n",
    "        # Dense-only model: flatten the sequence and use fully connected layers\n",
    "        inputs = tf.keras.Input(shape=(window_size * n_features,))  # input is a flat vector of length window_size*26\n",
    "        x = layers.Dense(128, activation='relu', kernel_initializer='he_normal')(inputs)\n",
    "        x = layers.Dense(64, activation='relu', kernel_initializer='he_normal')(x)\n",
    "        outputs = layers.Dense(n_outputs, activation='linear')(x)  # linear output for regression\n",
    "        model = models.Model(inputs, outputs)\n",
    "    \n",
    "    elif model_type == 'conv':\n",
    "        # Conv1D-only model: use convolutional layers over time dimension, then flatten to output\n",
    "        inputs = tf.keras.Input(shape=(window_size, n_features))   # input is a 5x26 sequence\n",
    "        x = layers.Conv1D(filters=64, kernel_size=3, activation='relu', \n",
    "                           kernel_initializer='he_normal', padding='same')(inputs)\n",
    "        x = layers.Conv1D(filters=64, kernel_size=3, activation='relu', \n",
    "                           kernel_initializer='he_normal', padding='same')(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        outputs = layers.Dense(n_outputs, activation='linear')(x)\n",
    "        model = models.Model(inputs, outputs)\n",
    "    \n",
    "    elif model_type == 'hybrid':\n",
    "        # Hybrid model: one Conv1D layer followed by Dense layers\n",
    "        inputs = tf.keras.Input(shape=(window_size, n_features))\n",
    "        x = layers.Conv1D(filters=64, kernel_size=3, activation='relu', \n",
    "                           kernel_initializer='he_normal', padding='same')(inputs)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(64, activation='relu', kernel_initializer='he_normal')(x)\n",
    "        outputs = layers.Dense(n_outputs, activation='linear')(x)\n",
    "        model = models.Model(inputs, outputs)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model_type. Choose from 'dense', 'conv', 'hybrid'.\")\n",
    "    \n",
    "    # Compile the model with optimizer, loss, and metrics for regression\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                  loss='mse',             # Mean Squared Error for regression loss\n",
    "                  metrics=['mae'])        # Mean Absolute Error as an additional metric\n",
    "    return model\n",
    "\n",
    "# Example: Build each type of model and display its architecture\n",
    "for arch in ['dense', 'conv', 'hybrid']:\n",
    "    model = create_model(model_type=arch, window_size=window_size, n_features=X_seq.shape[2], n_outputs=y_seq.shape[1])\n",
    "    print(f\"{arch.capitalize()} model summary:\")\n",
    "    model.summary()\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: The create_model function constructs a Keras model based on the specified model_type.\n",
    "For 'dense', we reshape the input into a vector of length window_size * n_features (for a window of 5 frames with 26 features each, that’s 130 inputs). We then add two Dense layers (128 and 64 units) with ReLU activations, and a final Dense layer with 13 units (one per joint's Z) and linear activation. Dropout can be added if needed, but here we keep it simple.\n",
    "For 'conv', the input shape is (window_size, n_features) (e.g., 5×26). We apply two 1D convolutional layers with a kernel size of 3 (looking at 3-frame patterns) and ‘same’ padding to preserve the sequence length. These conv layers will move a sliding 3-frame window over the 5-frame sequence to extract motion features. The output of conv layers is then flattened and fed directly to the output layer. (We could also add a Dense layer after flattening if desired, but here we're making conv-only relatively direct.)\n",
    "For 'hybrid', we use one Conv1D layer (to start learning temporal features), then flatten and pass through a Dense layer (64 units) before the final output. This combines the strengths of convolution (local pattern learning in time) and dense layers (global feature combination).\n",
    "In all cases, we compile the model with the Adam optimizer (a popular variant of stochastic gradient descent that adapts the learning rate for each parameter)​\n",
    "file-7q2hg52vvsvnwofjxdtetf\n",
    ". We use Mean Squared Error (MSE) as the loss function since this is a regression problem, and include Mean Absolute Error (MAE) as a metric for easier interpretation of errors. (MSE gives more weight to large errors, while MAE is more linear, so seeing both is useful.) After defining the function, we create one instance of each model type and print a summary of its architecture. The .summary() output shows the layers and shapes, which can help verify that each model is constructed as expected (you'll see the layer types Dense or Conv1D and the output shape of (None, 13) for the final layer, confirming 13 outputs).\n",
    "10-Fold Cross-Validation Training and Evaluation\n",
    "Now we will train and evaluate the model using 10-fold cross-validation. Cross-validation means we will split the dataset into 10 parts (folds) of roughly equal size. For each of the 10 iterations, we take one fold as the validation set and the remaining 9 folds as the training set. We train the model on the training portion and evaluate on the validation fold. This way, every data point gets to be in a validation set exactly once, and we can assess how well the model generalizes across all data. Using cross-validation is helpful especially when the dataset is not extremely large, as it maximizes the use of available data for training while still providing a robust evaluation on unseen data for each fold. It also helps to detect if the model is overfitting or if performance is consistent. Before we start, let's set up the model type we want to evaluate and prepare the KFold splitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which architecture to evaluate: 'dense', 'conv', or 'hybrid'\n",
    "model_type = 'conv'  # << change this to try a different architecture >>\n",
    "\n",
    "# Set up 10-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set shuffle=True for KFold to ensure the data is shuffled before splitting into folds (this is generally a good idea unless the data is time-series in order, but here shuffling across the entire dataset is fine since sequences are already formed and order within each sequence matters, not the global order of sequences). We use a random_state for reproducibility of the fold splits.\n",
    "Early Stopping and Model Checkpointing\n",
    "We'll use two Keras callbacks during training:\n",
    "EarlyStopping: This callback monitors the validation loss and stops training early if it hasn't improved in a specified number of epochs (patience). This prevents the model from overfitting the training data once it stops getting better on validation data. We will use restore_best_weights=True so that after stopping, the model weights revert to the best encountered during training (the epoch with lowest validation loss). We can adjust patience based on how many epochs of no-improvement we want to tolerate. For example, patience=5 will stop training if the validation loss doesn't improve for 5 consecutive epochs. If we wanted to disable early stopping, we could simply not include this callback, or set a very high patience (effectively letting all epochs run).​\n",
    "file-7q2hg52vvsvnwofjxdtetf\n",
    "ModelCheckpoint: This callback saves the model to disk whenever an improvement in the monitored metric (validation loss here) is observed. We set save_best_only=True to save only the best model (lowest val loss) for each fold. This is useful if we want to keep the model file for later use or analysis. We'll save each best model to a file like \"best_model.h5\". If you want to save models for each fold separately, you can include the fold number in the filename. If you prefer not to save models to disk, you can omit this callback.\n",
    "Now, let's perform the cross-validation loop. For each fold, we will:\n",
    "Split X_seq and y_seq into training and validation sets according to the current fold indices.\n",
    "Scale the features using StandardScaler: fit on the training data (flattening the sequence data into 2D shape of (samples*window_size, features) for scaling each feature across all time steps) and transform both training and validation data. We'll also scale the target y values (fit on y_train, transform y_val). This scaling is done inside each fold to avoid any data leakage.\n",
    "Build a fresh model for the chosen model_type (we need a new model for each fold to ensure independent training).\n",
    "Train the model on the training fold with early stopping and checkpoint callbacks, and validate on the validation fold. We set a relatively high epochs (e.g. 100) but expect early stopping to halt before that if the model stops improving. We also use a batch size of 32 (this can be tuned; 32 is a common starting point).\n",
    "Record the validation performance (MSE and MAE) for this fold.\n",
    "After the loop, compute the average MSE and MAE across all folds as the overall evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1 of 10...\n",
      " Fold 1 - Validation MSE: 0.0166, MAE: 0.0904\n",
      "Training fold 2 of 10...\n",
      " Fold 2 - Validation MSE: 0.0207, MAE: 0.1075\n",
      "Training fold 3 of 10...\n",
      " Fold 3 - Validation MSE: 0.0210, MAE: 0.1084\n",
      "Training fold 4 of 10...\n",
      " Fold 4 - Validation MSE: 0.0146, MAE: 0.0898\n",
      "Training fold 5 of 10...\n",
      " Fold 5 - Validation MSE: 0.0164, MAE: 0.0944\n",
      "Training fold 6 of 10...\n",
      " Fold 6 - Validation MSE: 0.0166, MAE: 0.0953\n",
      "Training fold 7 of 10...\n",
      " Fold 7 - Validation MSE: 0.0133, MAE: 0.0846\n",
      "Training fold 8 of 10...\n",
      " Fold 8 - Validation MSE: 0.0144, MAE: 0.0890\n",
      "Training fold 9 of 10...\n",
      " Fold 9 - Validation MSE: 0.0146, MAE: 0.0895\n",
      "Training fold 10 of 10...\n",
      " Fold 10 - Validation MSE: 0.0225, MAE: 0.1072\n",
      "\n",
      "Average 10-Fold Validation MSE: 0.017070140782743694\n",
      "Average 10-Fold Validation MAE: 0.09559105336666107\n"
     ]
    }
   ],
   "source": [
    "fold = 1\n",
    "val_scores = []  # to collect validation MSE and MAE for each fold\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_seq):\n",
    "    print(f\"Training fold {fold} of {kf.get_n_splits()}...\")\n",
    "    \n",
    "    # Split into training and validation for this fold\n",
    "    X_train_fold, X_val_fold = X_seq[train_idx], X_seq[val_idx]\n",
    "    y_train_fold, y_val_fold = y_seq[train_idx], y_seq[val_idx]\n",
    "    \n",
    "    # Scale features: fit on training fold, transform both train and val\n",
    "    X_scaler = StandardScaler()\n",
    "    y_scaler = StandardScaler()\n",
    "    # Flatten the 3D (samples, window_size, features) into 2D (samples*window_size, features) for scaling\n",
    "    X_train_flat = X_train_fold.reshape(-1, X_train_fold.shape[2])\n",
    "    X_val_flat = X_val_fold.reshape(-1, X_val_fold.shape[2])\n",
    "    X_train_flat_scaled = X_scaler.fit_transform(X_train_flat)\n",
    "    X_val_flat_scaled = X_scaler.transform(X_val_flat)\n",
    "    # Reshape back to the original sequence shape for model input\n",
    "    X_train_scaled = X_train_flat_scaled.reshape(X_train_fold.shape[0], X_train_fold.shape[1], X_train_fold.shape[2])\n",
    "    X_val_scaled = X_val_flat_scaled.reshape(X_val_fold.shape[0], X_val_fold.shape[1], X_val_fold.shape[2])\n",
    "    # Scale targets (Z values)\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train_fold)\n",
    "    y_val_scaled = y_scaler.transform(y_val_fold)\n",
    "    \n",
    "    # If using dense model, we need to flatten the sequence dimension for the model input\n",
    "    if model_type == 'dense':\n",
    "        X_train_input = X_train_scaled.reshape(X_train_scaled.shape[0], -1)  # flatten to (samples, window_size*n_features)\n",
    "        X_val_input   = X_val_scaled.reshape(X_val_scaled.shape[0], -1)\n",
    "    else:\n",
    "        X_train_input = X_train_scaled  # (samples, window_size, n_features)\n",
    "        X_val_input   = X_val_scaled\n",
    "    \n",
    "    # Create a new model for this fold\n",
    "    model = create_model(model_type=model_type, window_size=window_size, \n",
    "                         n_features=X_seq.shape[2], n_outputs=y_seq.shape[1])\n",
    "    \n",
    "    # Define callbacks\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, verbose=0)\n",
    "    \n",
    "    # Train the model on this fold's training data, with validation\n",
    "    history = model.fit(X_train_input, y_train_scaled, \n",
    "                        epochs=100, batch_size=32,\n",
    "                        validation_data=(X_val_input, y_val_scaled),\n",
    "                        callbacks=[early_stop, checkpoint],\n",
    "                        verbose=0)\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    val_loss, val_mae = model.evaluate(X_val_input, y_val_scaled, verbose=0)\n",
    "    val_scores.append((val_loss, val_mae))\n",
    "    print(f\" Fold {fold} - Validation MSE: {val_loss:.4f}, MAE: {val_mae:.4f}\")\n",
    "    fold += 1\n",
    "\n",
    "# Calculate average performance across all folds\n",
    "val_losses = [score[0] for score in val_scores]\n",
    "val_maes = [score[1] for score in val_scores]\n",
    "print(window_size, \"Frame Window\")\n",
    "print(\"\\nAverage 10-Fold Validation MSE:\", np.mean(val_losses))\n",
    "print(\"Average 10-Fold Validation MAE:\", np.mean(val_maes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: For each fold, we perform the steps discussed. We reshape and scale the data within the fold to ensure the scaler is fit only on training data. We conditionally reshape the inputs for the dense model (since it expects a flat input). We then compile a fresh model using our create_model function and train it.\n",
    "The EarlyStopping is set with patience=5, meaning if the validation loss doesn't improve for 5 epochs in a row, training will stop. We set restore_best_weights=True so the model will revert to the best weights achieved during this training when we evaluate it.\n",
    "The ModelCheckpoint will save the best model of the current fold to best_model.h5. After training each fold, you could load this file to examine or use the best model for that fold. Note that here we use the same filename for each fold, so it will be overwritten each time. If you want to save all fold models, you can specify filepath=f\"best_model_fold{fold}.h5\" in the callback.\n",
    "We run model.evaluate on the validation set to get the final MSE and MAE for that fold and store them. Finally, we compute the average MSE and MAE across all 10 folds to get an overall sense of performance. This average is a more stable estimate of how the model might perform on unseen data than a single train/test split. Note: We set verbose=0 in model.fit to suppress the per-epoch output for cleanliness (you can set verbose=1 to see the training progress for each fold). We do print a summary line per fold and the average at the end.\n",
    "Results and Next Steps\n",
    "After running the cross-validation above, you will have the average validation performance of the chosen architecture (model_type). For example, you might see output like:\n",
    "yaml\n",
    "Copy code\n",
    "Fold 1 - Validation MSE: 0.0123, MAE: 0.0805  \n",
    "...  \n",
    "Fold 10 - Validation MSE: 0.0131, MAE: 0.0832  \n",
    "\n",
    "Average 10-Fold Validation MSE: 0.0127  \n",
    "Average 10-Fold Validation MAE: 0.0810\n",
    "This indicates the model's typical prediction error. MAE ~0.0810 (in whatever units the Z is, possibly meters) means on average the prediction is off by about 0.081 in Z. You can now compare different architectures by setting model_type to 'conv' or 'hybrid' and re-running the cross-validation cell. Because we wrote our code modularly, trying a new architecture is as simple as changing that variable — no other code changes are needed. The function create_model takes care of building the appropriate model. By comparing the average MAE (or MSE) across folds for each architecture, you can determine which model type performs best for this task. Early Stopping and Model Checkpoint Recap: During training, if you notice that training stops well before the max epochs (due to early stopping), that's usually a good sign that the model found an optimum. If it stops too early (possibly underfitting), you might consider increasing patience or initial epochs. If it never stops (goes to full epochs) and validation loss keeps improving, you might increase epochs or adjust learning rate, or reduce patience if training is very long. The model checkpoint ensures you have the best model weights; since we used restore_best_weights=True, the model you evaluate is already the best one, so loading from the checkpoint isn't strictly necessary unless you want to save it for later use. To disable these, simply remove them from the callbacks list. Extensibility: Feel free to experiment further:\n",
    "Adjust the window_size for the input sequence (e.g., try 3 or 7) and see if more or less temporal context helps.\n",
    "Modify the network architecture: you can add more Dense layers, change the number of filters in Conv1D, or even try other sequence models like LSTM/GRU for comparison.\n",
    "Tune hyperparameters such as learning rate, batch size, or training epochs to see their effect on performance.\n",
    "If you have a separate test set, after choosing the best model type and training on all data (or all training data), evaluate the model on that test set to get a final unbiased performance metric.\n",
    "This notebook provides a clear, modular framework for tackling the Kinect XY→Z prediction problem, making it easier for beginners to understand and adjust various components of the deep learning pipeline. Happy experimenting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

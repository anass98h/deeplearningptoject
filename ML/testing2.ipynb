{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kinect XY to Z Prediction Experiment Pipeline\n",
    "Introduction\n",
    "We aim to predict the Kinect sensor's Z-coordinate (depth) for each of 13 body joints using only the X and Y coordinates as input. The Kinect provides 3D positions (X, Y, Z) for each joint across a sequence of frames. By leveraging temporal information, we can improve prediction accuracy: instead of predicting frame-by-frame independently, we use a sliding window of consecutive frames as input, so the model can learn from motion context in recent frames​\n",
    "file-1fjmgdc2axfxcweubcedh3\n",
    ". The task is a multi-output regression – for each window of frames, we predict 13 Z-values (one per joint) corresponding to the last frame in the window. We will experiment with various neural network architectures (fully-connected dense networks, 1D convolutional networks, LSTM recurrent networks, and hybrid combinations) to determine what works best for this sequence modeling problem. Key considerations in our experimental pipeline include:\n",
    "Using a sliding window per sequence without crossing sequence boundaries (each CSV file is one sequence) to generate training samples.\n",
    "Implementing 10-fold cross-validation for robust evaluation: training on 9 folds and testing on 1 fold, rotating through all folds​\n",
    "file-1fjmgdc2axfxcweubcedh3\n",
    ".\n",
    "Conducting a comprehensive grid search over hyperparameters: window size, learning rate, architecture type, number of layers, and units per layer.\n",
    "Employing GPU acceleration (e.g. an NVIDIA RTX 3060 12GB) to speed up training. We'll use TensorFlow/Keras which automatically utilizes available GPUs for heavy tensor computations.\n",
    "Using early stopping to halt training when validation loss stops improving (to prevent overfitting and save time) and model checkpointing to save the best model weights​\n",
    "file-1fjmgdc2axfxcweubcedh3\n",
    " for each fold.\n",
    "Logging the results (fold metrics, hyperparameters, training time) incrementally to a CSV file for analysis.\n",
    "By the end of this pipeline, we will have a CSV record of the performance of each configuration, which can be analyzed to find the optimal model.\n",
    "Approach Outline\n",
    "Data Loading – Load all Kinect CSV files. Each file contains one sequence of frames with 13 joints' coordinates. We combine these into a list of sequences. We drop any unnecessary columns (like frame index) and separate features vs. target.\n",
    "Data Preprocessing – For each frame, use X and Y coordinates as input features and the Z coordinates as target outputs​\n",
    "file-1fjmgdc2axfxcweubcedh3\n",
    "​\n",
    "file-1fjmgdc2axfxcweubcedh3\n",
    ". Normalize feature values for better training convergence.\n",
    "Sliding Window Creation – Define a procedure to generate sliding window samples from each sequence. For a given window size W, each sample consists of W consecutive frames' XY inputs and the Z outputs of the last frame in that window.\n",
    "Model Definition – Implement a function to build a TensorFlow model given a specific architecture type and hyperparameters. We will support:\n",
    "Dense (fully-connected): Flatten the window input and use a stack of Dense layers​\n",
    "file-1fjmgdc2axfxcweubcedh3\n",
    ".\n",
    "Conv1D: Use 1D convolution layers across the time dimension of the window to capture motion patterns, then flatten.\n",
    "LSTM (recurrent): Use LSTM layers to capture temporal dependencies in the sequence.\n",
    "Hybrid (Conv + Dense): Use some Conv1D layers followed by Dense layers (after flattening) – a combination of convolution for local feature extraction and dense layers for mixing features.\n",
    "CNN+LSTM: Use Conv1D layer(s) to extract low-level temporal features, then an LSTM layer on top to capture longer-term dependencies.\n",
    "All hidden layers will use ReLU activations with He initialization (suitable for ReLU)​\n",
    "file-1fjmgdc2axfxcweubcedh3\n",
    ". The output layer will be linear (no activation) to predict raw Z values. We use the Adam optimizer (a variant of SGD) with Mean Squared Error loss for regression and will track Mean Absolute Error (MAE) as an additional metric​\n",
    "file-1fjmgdc2axfxcweubcedh3\n",
    ".\n",
    "Cross-Validation Training – For each combination of hyperparameters, perform 10-fold cross-validation​\n",
    "file-1fjmgdc2axfxcweubcedh3\n",
    ":\n",
    "Split the sequences into 10 folds. In each fold, use 9/10 of sequences for training (and validation) and 1/10 for testing.\n",
    "Within training folds, further split a validation subset or use a portion of training data for early stopping monitoring.\n",
    "EarlyStopping: Monitor validation loss and stop training if it doesn't improve for a set patience (we'll use patience=5)​\n",
    "file-1fjmgdc2axfxcweubcedh3\n",
    ".\n",
    "ModelCheckpoint: Save the best weights for the model on the validation set during training​\n",
    "file-1fjmgdc2axfxcweubcedh3\n",
    ".\n",
    "After training each fold, evaluate on that fold's test data and record the MSE/MAE.\n",
    "Clear the model and GPU memory before the next fold to avoid memory buildup.\n",
    "Hyperparameter Grid Search – Loop over all combinations of the specified hyperparameter ranges (window sizes, learning rates, architecture types, number of layers, units per layer, etc.). For each configuration, run the 10-fold CV training as above, then log the results (per-fold metrics, average performance, training time).\n",
    "Logging and Analysis – Incrementally append the results of each experiment to a CSV file. After the grid search, this file will contain the performance of each model variant. We can then analyze this CSV to find which hyperparameters gave the best results (e.g. lowest average MSE).\n",
    "With this plan, let's proceed step by step.\n",
    "1. Import Libraries and Configure GPU\n",
    "First, we import the necessary libraries. We use pandas for data loading/manipulation, NumPy for numerical computations, scikit-learn for data splitting (KFold) and feature scaling, and TensorFlow/Keras for building and training the neural networks​\n",
    "file-1fjmgdc2axfxcweubcedh3\n",
    ". We also enable dynamic growth for GPU memory to avoid TensorFlow pre-allocating all VRAM at once (this helps to run many models sequentially without memory fragmentation issues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Using CPU for training.\n",
      "TensorFlow version: 2.19.0\n",
      "GPUs: []\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Configure TensorFlow to use GPU efficiently (if available)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print(\"GPU found. Enabled memory growth on\", gpus[0].name)\n",
    "    except Exception as e:\n",
    "        print(\"Error enabling GPU memory growth:\", e)\n",
    "else:\n",
    "    print(\"No GPU found. Using CPU for training.\")\n",
    "    print(\"TensorFlow version:\", tf.__version__)\n",
    "    print(\"GPUs:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: We import all necessary modules. The set_memory_growth option allows the program to only allocate GPU memory as needed, which is useful when training many models sequentially. If no GPU is present, TensorFlow will automatically fall back to CPU.\n",
    "2. Data Loading and Preprocessing\n",
    "Each Kinect CSV file contains one sequence of recorded joint coordinates. We assume all such CSV files are stored in a directory (e.g., \"data/kinect_sequences\"). We will load each file into a DataFrame and then combine them into a list of sequences. Each sequence DataFrame has columns for each joint's x, y, z coordinates (39 columns for 13 joints, plus maybe an index)​\n",
    "file-1fjmgdc2axfxcweubcedh3\n",
    ". We drop the frame number/index column since it is not a feature for learning. Then we separate the input features (all *_x and *_y columns) and the target outputs (all *_z columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing Kinect CSV files\n",
    "data_dir = \"data/kinect_sequences\"  # replace with your actual path\n",
    "csv_files = sorted([f for f in os.listdir(data_dir) if f.endswith(\".csv\")])\n",
    "\n",
    "# Lists to hold sequences of features (X) and targets (Z)\n",
    "sequences_X = []\n",
    "sequences_Z = []\n",
    "\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    try:\n",
    "        # Load the CSV into a pandas DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Successfully loaded {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Drop frame number column if present\n",
    "    if 'FrameNo' in df.columns or 'frame' in df.columns:\n",
    "        df = df.drop(columns=['FrameNo'], errors='ignore')\n",
    "        df = df.drop(columns=['frame'], errors='ignore')\n",
    "    \n",
    "    # Strip any whitespace from column names (if needed)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Separate feature and target columns\n",
    "    feature_cols = [col for col in df.columns if col.endswith('_x') or col.endswith('_y')]\n",
    "    target_cols = [col for col in df.columns if col.endswith('_z')]\n",
    "    \n",
    "    X_values = df[feature_cols].to_numpy()\n",
    "    Z_values = df[target_cols].to_numpy()\n",
    "    \n",
    "    sequences_X.append(X_values)\n",
    "    sequences_Z.append(Z_values)\n",
    "\n",
    "print(f\"Loaded {len(sequences_X)} sequences. Example sequence shape: {sequences_X[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we iterate through each CSV file:\n",
    "Read it into a DataFrame.\n",
    "Drop the frame number column (FrameNo) if it exists.\n",
    "Identify feature columns (those ending in _x or _y) and target columns (ending in _z).\n",
    "Convert those to NumPy arrays and append to our lists. For example, sequences_X[i] will be a NumPy array of shape (num_frames_i, 26) since there are 26 features (13 joints * 2 coordinates), and sequences_Z[i] will be (num_frames_i, 13) with 13 target values per frame.\n",
    "After loading, sequences_X is a list of length N (number of sequences), where each element is an array of shape (frames_in_sequence, 26). Similarly, sequences_Z contains arrays of shape (frames_in_sequence, 13). Feature Scaling: It is often beneficial to normalize or standardize input features for neural network training so that all features are on a similar scale. We will use standardization (zero mean, unit variance) for the X and Y inputs. Important: To avoid data leakage, scaling will be fit separately for each training fold (using only that fold’s training data) and then applied to the validation/test data. We will handle this inside the cross-validation loop later. (Targets (Z) could also be scaled, but here we'll predict the actual Z values directly. Since all coordinates are in similar units, we can leave Z unscaled for interpretability.)\n",
    "3. Sliding Window Data Generation\n",
    "We need to transform each sequence of frames into training samples using a sliding window approach. For a chosen window size W, we generate all possible contiguous windows of length W from a sequence. Each such window will produce one training example:\n",
    "Input (features): The X and Y coordinates for all 13 joints over the W frames (shape W×26).\n",
    "Output (target): The Z coordinates for all 13 joints from the last frame of that window (shape 13, one value per joint).\n",
    "We do not allow windows to wrap around or cross between sequences – windows are generated independently within each sequence. If a sequence has T frames, and window size is W, it will yield (T - W + 1) samples (if T >= W, otherwise zero samples if the sequence is shorter than the window). Let's implement a utility function to create sliding window samples for a given sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows_from_sequence(X_seq, Z_seq, window_size):\n",
    "    \"\"\"\n",
    "    Given a single sequence of features X_seq (shape: num_frames x 26) and \n",
    "    targets Z_seq (shape: num_frames x 13), generate all sliding window samples of length window_size.\n",
    "    Returns:\n",
    "      X_windows: array of shape (num_samples, window_size, 26)\n",
    "      Y_windows: array of shape (num_samples, 13) corresponding to Z of last frame in each window\n",
    "    \"\"\"\n",
    "    X_windows = []\n",
    "    Y_windows = []\n",
    "    num_frames = X_seq.shape[0]\n",
    "    if num_frames < window_size:\n",
    "        # Not enough frames for even one window\n",
    "        return np.array(X_windows), np.array(Y_windows)\n",
    "    for start in range(0, num_frames - window_size + 1):\n",
    "        end = start + window_size\n",
    "        # Stack frames [start, ..., end-1] as one window\n",
    "        X_w = X_seq[start:end]              # shape (window_size, 26)\n",
    "        Y_w = Z_seq[end-1]                 # shape (13,) - Z of last frame\n",
    "        X_windows.append(X_w)\n",
    "        Y_windows.append(Y_w)\n",
    "    # Convert to numpy arrays\n",
    "    X_windows = np.array(X_windows)\n",
    "    Y_windows = np.array(Y_windows)\n",
    "    return X_windows, Y_windows\n",
    "\n",
    "# Quick test on the first sequence (using a small window for demonstration)\n",
    "test_w = 5\n",
    "X_test_win, Y_test_win = create_windows_from_sequence(sequences_X[0], sequences_Z[0], window_size=test_w)\n",
    "print(f\"Created {X_test_win.shape[0]} window samples from one sequence (window={test_w}).\")\n",
    "print(\"Sample window input shape:\", X_test_win[0].shape, \"/ Sample window output shape:\", Y_test_win[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: The function create_windows_from_sequence iterates through the sequence with a window of length window_size. For each starting index, it slices out a window of the feature array X_seq and takes the corresponding last frame's target from Z_seq. It returns arrays of all windowed inputs and outputs. We included a quick test print to ensure it works (the output should show the number of samples generated and the shapes).\n",
    "4. Model Architecture Definition\n",
    "Now we define a function to build a Keras model given a specific architecture type and hyperparameters. This function will allow us to dynamically create models for each combination in our grid search. The parameters will include:\n",
    "arch: The architecture type (one of 'dense', 'conv1d', 'lstm', 'hybrid', 'cnn+lstm').\n",
    "num_layers: Number of hidden layers to use (excluding the output layer). For different architectures this means:\n",
    "Dense: number of Dense hidden layers.\n",
    "Conv1D: number of Conv1D layers.\n",
    "LSTM: number of LSTM layers (they will be stacked if >1).\n",
    "Hybrid (Conv + Dense): we will use num_layers // 2 Conv layers and num_layers - (num_layers // 2) Dense layers (approximately half conv, half dense).\n",
    "CNN+LSTM: we will use (num_layers - 1) Conv1D layers followed by 1 LSTM layer (at least one conv layer before the LSTM).\n",
    "units: Number of units/filters in each hidden layer. (For simplicity, we use the same size for all layers of a given model.)\n",
    "learning_rate: Learning rate for the optimizer.\n",
    "All models will have an input shape of (window_size, 26) and output size of 13. We will use ReLU activation for hidden layers and linear activation for the output layer (since this is a regression problem). We also initialize Dense/Conv weights with He initialization (Keras uses kernel_initializer='he_uniform' for ReLU layers)​\n",
    "file-1fjmgdc2axfxcweubcedh3\n",
    ". The models will be compiled with Adam optimizer and Mean Squared Error loss​\n",
    "file-1fjmgdc2axfxcweubcedh3\n",
    ", and we'll track Mean Absolute Error (MAE) as a metric for easier interpretation of error.\n",
    "python\n",
    "Copy code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(window_size, arch, num_layers, units, learning_rate):\n",
    "    \"\"\"Constructs and compiles a Keras model given the architecture and hyperparameters.\"\"\"\n",
    "    model = models.Sequential()\n",
    "    # Define input shape (window_size timesteps, 26 features per timestep)\n",
    "    model.add(layers.Input(shape=(window_size, 26)))\n",
    "    \n",
    "    if arch == 'dense':\n",
    "        # Flatten time dimension and use Dense layers\n",
    "        model.add(layers.Flatten())  # shape becomes (window_size*26,)\n",
    "        for _ in range(num_layers):\n",
    "            model.add(layers.Dense(units, activation='relu', kernel_initializer='he_uniform'))\n",
    "        # Output layer\n",
    "        model.add(layers.Dense(13, activation='linear'))\n",
    "    \n",
    "    elif arch == 'conv1d':\n",
    "        # Conv1D layers across time dimension\n",
    "        for _ in range(num_layers):\n",
    "            model.add(layers.Conv1D(filters=units, kernel_size=3, padding='same',\n",
    "                                     activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(13, activation='linear'))\n",
    "    \n",
    "    elif arch == 'lstm':\n",
    "        # Stacked LSTM layers\n",
    "        for i in range(num_layers):\n",
    "            # If not the last LSTM layer, return sequences to feed next LSTM\n",
    "            return_seq = (i < num_layers - 1)\n",
    "            model.add(layers.LSTM(units, return_sequences=return_seq))\n",
    "        model.add(layers.Dense(13, activation='linear'))\n",
    "    \n",
    "    elif arch == 'hybrid':\n",
    "        # Combination of Conv1D and Dense layers\n",
    "        conv_count = num_layers // 2\n",
    "        dense_count = num_layers - conv_count\n",
    "        # Conv layers first\n",
    "        for _ in range(conv_count):\n",
    "            model.add(layers.Conv1D(filters=units, kernel_size=3, padding='same',\n",
    "                                     activation='relu', kernel_initializer='he_uniform'))\n",
    "        if conv_count > 0:\n",
    "            model.add(layers.Flatten())\n",
    "        # Dense layers after conv\n",
    "        for _ in range(dense_count):\n",
    "            model.add(layers.Dense(units, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(layers.Dense(13, activation='linear'))\n",
    "    \n",
    "    elif arch == 'cnn+lstm':\n",
    "        # Conv layers followed by a single LSTM layer\n",
    "        conv_layers = max(1, num_layers - 1)  # ensure at least 1 conv\n",
    "        for _ in range(conv_layers):\n",
    "            model.add(layers.Conv1D(filters=units, kernel_size=3, padding='same',\n",
    "                                     activation='relu', kernel_initializer='he_uniform'))\n",
    "        # Follow with one LSTM layer\n",
    "        model.add(layers.LSTM(units, return_sequences=False))\n",
    "        model.add(layers.Dense(13, activation='linear'))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown architecture type: {arch}\")\n",
    "    \n",
    "    # Compile the model with Adam optimizer and MSE loss\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Test the model creation function for one example configuration\n",
    "test_model = create_model(window_size=5, arch='conv1d', num_layers=3, units=64, learning_rate=0.001)\n",
    "print(test_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the create_model function:\n",
    "For Dense architecture, we flatten the input (concatenating all frames in the window) and then add the specified number of Dense layers.\n",
    "For Conv1D, we add the specified number of Conv1D layers. We use padding='same' so that each conv layer preserves the time dimension length, making it easier to stack conv layers. After conv layers, we flatten and then have a Dense output.\n",
    "For LSTM, we add num_layers LSTM layers. Each LSTM (except the last) is set to return_sequences=True so that the next LSTM receives a sequence input. The final LSTM returns the last output (a feature vector), which we feed into the Dense output.\n",
    "For Hybrid, we split the layers between Conv and Dense. We take roughly half of the layers as Conv1D (rounded down) and the rest as Dense. After the Conv layers, we flatten and then add the Dense layers. (If num_layers is odd, e.g. 5, this will produce 2 conv and 3 dense layers, etc.)\n",
    "For CNN+LSTM, we use all but one layers as Conv1D, and then a single LSTM at the end. For example, if num_layers=3, we'll have 2 Conv1D layers followed by 1 LSTM layer. If num_layers=1, our implementation uses at least one Conv layer (since conv_layers = max(1, num_layers-1) will be 1) and then an LSTM – effectively the same as a small CNN+LSTM with 1 conv, 1 lstm.\n",
    "All models end with a Dense layer of 13 units (linear activation) for the output, matching the 13 target joint Z-coordinates. We printed a summary of a test model to verify the architecture (in practice this will show the layer types and output shapes for the given configuration).\n",
    "5. Training with Cross-Validation and Hyperparameter Search\n",
    "With data loading, preprocessing, and model definition in place, we now set up the grid search over hyperparameters and perform 10-fold cross-validation for each combination. Hyperparameters to grid-search:\n",
    "Window sizes: [3, 5, 7, 9, 11, 13, 15, 17, 20]\n",
    "Learning rates: [0.5, 0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
    "Architectures: ['dense', 'conv1d', 'hybrid', 'lstm', 'cnn+lstm']\n",
    "Number of layers: [2, 3, 4, 5, 6, 8, 10, 12] (recall: for conv, dense, etc., this is the hidden layer count as defined earlier)\n",
    "Units per layer: [64, 128, 256, 512]\n",
    "We will use 10-fold cross-validation for each combination. We create a KFold splitter to generate train/test sequence splits. Each fold, we will:\n",
    "Determine which sequences are in the training set and which are in the test set for that fold.\n",
    "Fit a StandardScaler on the training set's feature data (all frames from the training sequences) and transform the features of both training and test data. This ensures the model is trained on standardized inputs, and the test inputs are scaled consistently.\n",
    "Generate sliding window samples for all training sequences (these become our training data for the model) and for the test sequences (for evaluation). Important: We will generate the windows after scaling the features so that each sample is properly normalized.\n",
    "Split a portion of the training windows off for validation (to monitor early stopping). We can use, for example, 10% of the training windows as a validation set.\n",
    "Create the model for the current hyperparameter combo using create_model.\n",
    "Train the model on the training windows, with EarlyStopping and ModelCheckpoint callbacks:\n",
    "EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) will stop training if the validation loss hasn't improved in 5 epochs and revert to the best weights.\n",
    "ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True, save_weights_only=True) will save the model's weights at the epoch where validation loss was best.\n",
    "After training, evaluate the model on the test fold's data to get the performance (MSE and MAE).\n",
    "Clear the model from memory (and clear the session) to prepare for the next fold.\n",
    "Repeat for all 10 folds and compute the average performance.\n",
    "After all folds for a given hyperparameter combination, record the results (fold-wise metrics and averages) along with the hyperparams and training time. Then proceed to the next combination. This is a computationally intensive process (training thousands of models if all combinations are run). Thanks to GPU acceleration and the relatively small size of each model, this is feasible, but it may still take a long time. Early stopping will help by cutting off training when further epochs yield no improvement. Let's implement the nested loops for the grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "window_sizes = [3, 5, 7, 9, 11, 13, 15, 17, 20]\n",
    "learning_rates = [0.5, 0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
    "architectures = ['dense', 'conv1d', 'hybrid', 'lstm', 'cnn+lstm']\n",
    "num_layers_list = [2, 3, 4, 5, 6, 8, 10, 12]\n",
    "units_list = [64, 128, 256, 512]\n",
    "epochs_per_fold = 50\n",
    "\n",
    "# Prepare K-fold splitter (we will shuffle the sequence indices for randomness)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# CSV file to log results\n",
    "results_file = \"experiment_results.csv\"\n",
    "# Write CSV header\n",
    "results_header = ([\"architecture\", \"window_size\", \"num_layers\", \"units\", \"learning_rate\"] +\n",
    "                  [f\"fold{i+1}_mse\" for i in range(10)] +\n",
    "                  [\"avg_mse\", \"avg_mae\", \"training_time_sec\"])\n",
    "with open(results_file, 'w') as f:\n",
    "    f.write(\",\".join(results_header) + \"\\n\")\n",
    "\n",
    "# Begin grid search\n",
    "import time\n",
    "experiment_count = 0\n",
    "total_experiments = (len(window_sizes) * len(learning_rates) * \n",
    "                     len(architectures) * len(num_layers_list) * len(units_list))\n",
    "print(f\"Total experiments to run: {total_experiments}\")\n",
    "for arch in architectures:\n",
    "    for window_size in window_sizes:\n",
    "        for num_layers in num_layers_list:\n",
    "            for units in units_list:\n",
    "                for lr in learning_rates:\n",
    "                    experiment_count += 1\n",
    "                    config_description = (f\"arch={arch}, window={window_size}, layers={num_layers}, \"\n",
    "                                           f\"units={units}, lr={lr}\")\n",
    "                    print(f\"\\n=== Experiment {experiment_count}/{total_experiments}: {config_description} ===\")\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    fold_mse_scores = []\n",
    "                    fold_mae_scores = []\n",
    "                    \n",
    "                    # Perform 10-fold cross-validation for this config\n",
    "                    fold_index = 1\n",
    "                    for train_idx, test_idx in kf.split(sequences_X):\n",
    "                        # Prepare training and testing data for this fold\n",
    "                        # Combine training sequences' frames to fit scaler\n",
    "                        train_frames = []\n",
    "                        for seq_idx in train_idx:\n",
    "                            train_frames.append(sequences_X[seq_idx])\n",
    "                        train_frames = np.vstack(train_frames)\n",
    "                        # Fit scaler on all training frames (for X features)\n",
    "                        scaler = StandardScaler().fit(train_frames)\n",
    "                        \n",
    "                        # Generate windowed data for training\n",
    "                        X_train_all = []\n",
    "                        Y_train_all = []\n",
    "                        for seq_idx in train_idx:\n",
    "                            # Scale the entire sequence's features\n",
    "                            X_seq = scaler.transform(sequences_X[seq_idx])\n",
    "                            Z_seq = sequences_Z[seq_idx]  # target can remain unscaled\n",
    "                            X_wins, Y_wins = create_windows_from_sequence(X_seq, Z_seq, window_size)\n",
    "                            if X_wins.size == 0:\n",
    "                                continue  # sequence too short for this window size\n",
    "                            X_train_all.append(X_wins)\n",
    "                            Y_train_all.append(Y_wins)\n",
    "                        if len(X_train_all) == 0:\n",
    "                            # If no training data (should not happen unless window_size > all seq lengths)\n",
    "                            continue\n",
    "                        # Concatenate all training windows from all sequences\n",
    "                        X_train_all = np.vstack(X_train_all)\n",
    "                        Y_train_all = np.vstack(Y_train_all)\n",
    "                        \n",
    "                        # Generate windowed data for testing (evaluation fold)\n",
    "                        X_test_all = []\n",
    "                        Y_test_all = []\n",
    "                        for seq_idx in test_idx:\n",
    "                            X_seq = scaler.transform(sequences_X[seq_idx])\n",
    "                            Z_seq = sequences_Z[seq_idx]\n",
    "                            X_wins, Y_wins = create_windows_from_sequence(X_seq, Z_seq, window_size)\n",
    "                            if X_wins.size == 0:\n",
    "                                continue  # If test sequence too short, it contributes no samples\n",
    "                            X_test_all.append(X_wins)\n",
    "                            Y_test_all.append(Y_wins)\n",
    "                        if len(X_test_all) == 0:\n",
    "                            # If no test data for this fold (all test sequences too short), skip fold\n",
    "                            print(f\"Fold {fold_index}: no test data (sequence too short for window={window_size}). Skipping.\")\n",
    "                            continue\n",
    "                        X_test_all = np.vstack(X_test_all)\n",
    "                        Y_test_all = np.vstack(Y_test_all)\n",
    "                        \n",
    "                        # Split off a validation set from training data for early stopping\n",
    "                        X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "                            X_train_all, Y_train_all, test_size=0.1, random_state=42)\n",
    "                        \n",
    "                        # Build model for this configuration\n",
    "                        model = create_model(window_size, arch, num_layers, units, lr)\n",
    "                        \n",
    "                        # Callbacks for early stopping and checkpointing\n",
    "                        callbacks = [\n",
    "                            EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "                            ModelCheckpoint(filepath='best_model.h5', monitor='val_loss',\n",
    "                                            save_best_only=True, save_weights_only=True)\n",
    "                        ]\n",
    "                        \n",
    "                        # Train the model\n",
    "                        model.fit(X_train, Y_train, epochs=epochs_per_fold, batch_size=32,\n",
    "                                  validation_data=(X_val, Y_val), callbacks=callbacks, verbose=0)\n",
    "                        \n",
    "                        # Load best weights (if not already restored by early stopping)\n",
    "                        # (EarlyStopping with restore_best_weights=True already did this, but we'll ensure by loading checkpoint)\n",
    "                        try:\n",
    "                            model.load_weights('best_model.h5')\n",
    "                        except Exception as e:\n",
    "                            pass  # If file not found (e.g., not saved because no improvement), ignore\n",
    "                        \n",
    "                        # Evaluate on the test set\n",
    "                        loss, mae = model.evaluate(X_test_all, Y_test_all, verbose=0)\n",
    "                        fold_mse_scores.append(loss)\n",
    "                        fold_mae_scores.append(mae)\n",
    "                        print(f\"Fold {fold_index} MSE: {loss:.6f}, MAE: {mae:.6f}\")\n",
    "                        \n",
    "                        # Clean up model to free memory before next fold\n",
    "                        tf.keras.backend.clear_session()\n",
    "                        del model\n",
    "                        fold_index += 1\n",
    "                    \n",
    "                    # Compute average metrics across folds\n",
    "                    avg_mse = float(np.mean(fold_mse_scores)) if fold_mse_scores else float('nan')\n",
    "                    avg_mae = float(np.mean(fold_mae_scores)) if fold_mae_scores else float('nan')\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(f\"Avg MSE: {avg_mse:.6f}, Avg MAE: {avg_mae:.6f}, Training time: {elapsed:.2f} sec\")\n",
    "                    \n",
    "                    # Log results to CSV\n",
    "                    result_data = [arch, window_size, num_layers, units, lr]\n",
    "                    # Add each fold's MSE\n",
    "                    for i in range(10):\n",
    "                        result_data.append(fold_mse_scores[i] if i < len(fold_mse_scores) else \"\")\n",
    "                    result_data += [avg_mse, avg_mae, elapsed]\n",
    "                    result_line = \",\".join(map(str, result_data))\n",
    "                    with open(results_file, 'a') as f:\n",
    "                        f.write(result_line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the training loop: We iterate over every combination of the hyperparameters. For each combination (arch, window_size, num_layers, units, lr):\n",
    "We initialize lists to collect the MSE and MAE for each of the 10 folds.\n",
    "We use KFold(n_splits=10, shuffle=True) to get train/test splits at the sequence level (not individual frames). train_idx and test_idx are indices of sequences for each fold.\n",
    "For each fold:\n",
    "We gather all training sequences' feature frames into one array train_frames and fit a StandardScaler on it. This computes the mean and std of each feature (each joint's X or Y across all frames in the training sequences).\n",
    "We then scale each training sequence's features and generate window samples from it using create_windows_from_sequence. We collect all training windows in X_train_all and Y_train_all.\n",
    "We do the same for test sequences to get X_test_all and Y_test_all for evaluation.\n",
    "We perform a train/validation split on the training windows (10% used for validation) using train_test_split. This validation set is used for monitoring early stopping.\n",
    "We create a new model for this fold with the current hyperparams by calling create_model(window_size, arch, ...).\n",
    "We set up the callbacks:\n",
    "EarlyStopping: monitors validation loss (val_loss which is MSE) with patience 5. It also has restore_best_weights=True so that after stopping, the model weights revert to the best seen.\n",
    "ModelCheckpoint: saves the best model weights to 'best_model.h5' during training. We use save_best_only=True so it updates only when a better val_loss is found.\n",
    "We train the model with model.fit for up to 50 epochs (as specified by epochs_per_fold), using a batch size of 32. We pass in the validation data for monitoring. We set verbose=0 to suppress per-epoch output (to keep logs cleaner).\n",
    "After training, we load the best weights from the checkpoint file (just to be absolutely sure we have the best model – in practice, because we used restore_best_weights, the model is already at best state).\n",
    "We then evaluate the model on the test set (model.evaluate) to get the MSE (loss) and MAE for this fold. These are appended to our fold_mse_scores and fold_mae_scores lists.\n",
    "We print the fold results for monitoring progress.\n",
    "We clear the Keras session and delete the model to free GPU memory before starting the next fold (this is critical in a large loop to avoid out-of-memory errors).\n",
    "After all 10 folds, we compute the average MSE and MAE across the folds.\n",
    "We also measure the total training time for this configuration (from before the first fold to after the last fold) using time.time().\n",
    "We log a summary line for this experiment to experiment_results.csv. The logged data includes the hyperparameters, each fold's MSE, the average MSE, average MAE, and the training time in seconds.\n",
    "We open the results file in append mode for each experiment so that if the process is interrupted, we still have results up to the last completed configuration. The CSV header was written once at the top of the file (with columns for each fold's MSE, etc.). During execution, we print progress messages including the current experiment number and hyperparameters, fold metrics, and the average results. This will help track the progress in the console since the entire grid search will take a long time. Note: There are conditions to skip a fold if no data is available (for example, if a window size is larger than all sequences, some folds may have no training or no testing samples). In such cases we skip logging that fold's result. In a well-prepared dataset (with sequences longer than the largest window), this should not occur.\n",
    "6. Running the Experiments and Logging Results\n",
    "After setting up the loop above, running that code will commence the grid search. Given the number of combinations (9 window sizes × 6 learning rates × 5 architectures × 8 layer counts × 4 unit sizes = 8640 experiments, each with 10-fold training), this is a lot of training. It could potentially take many hours or days to run to completion on a single GPU. In practice, you might want to reduce the search space or distribute the task. However, for demonstration, the code as written will log results incrementally. For example, the CSV file might start to look like this (with made-up numbers for illustration):\n",
    "python-repl\n",
    "Copy code\n",
    "architecture,window_size,num_layers,units,learning_rate,fold1_mse,fold2_mse,...,fold10_mse,avg_mse,avg_mae,training_time_sec\n",
    "dense,3,2,64,0.5,0.0123,0.0108,...,0.0115,0.0115,0.085,45.2\n",
    "dense,3,2,64,0.01,0.0056,0.0061,...,0.0059,0.0059,0.065,48.7\n",
    "...\n",
    "Each line corresponds to one configuration. This allows us to later analyze which combination gave the lowest average MSE or MAE. (In a real scenario, one might also consider logging the standard deviation of fold errors, or using a more compact metric like average RMSE, but here we stick to MSE/MAE as specified.)\n",
    "7. Conclusion and Next Steps\n",
    "We have built a GPU-accelerated deep learning experimentation pipeline that systematically evaluates different models for predicting joint depth (Z) from 2D coordinates. By using a sliding window of frames as input, the models utilize temporal context to make more accurate predictions​\n",
    "file-1fjmgdc2axfxcweubcedh3\n",
    ". We incorporated fully connected networks, convolutional networks, recurrent LSTMs, and hybrid combinations, with extensive hyperparameter tuning for each. The use of 10-fold cross-validation provides a robust estimate of performance for each configuration, and techniques like early stopping and checkpointing ensure that training is efficient and avoids overfitting​\n",
    "file-1fjmgdc2axfxcweubcedh3\n",
    ". All results are logged in experiment_results.csv. The next step is to analyze the results:\n",
    "Identify the model configuration with the lowest average MSE (or MAE) on the cross-validation folds.\n",
    "Consider the trade-offs (model complexity vs. performance). For example, a complex model (many layers/units) might yield slightly better accuracy but take longer to train or risk overfitting, whereas a simpler model might be nearly as good and more efficient.\n",
    "Once the best hyperparameter combination is identified, we could train a final model on the entire dataset using that configuration (perhaps with a new train/test split or on a separate test set if available) to confirm its performance.\n",
    "We could also examine the errors for different joints or frames to see if certain joints' depth are harder to predict from 2D (which might indicate limits of the approach or the need for more sophisticated temporal modeling).\n",
    "Overall, this pipeline provides a powerful framework for experimentation. By logging every experiment, it enables a thorough grid search and ensures reproducibility of results for this Kinect XY-to-Z prediction task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kinect XY to Z Prediction Experiment Pipeline\n",
    "In this experiment, we predict the Kinect sensor's Z-coordinate for 13 body joints using only their X and Y coordinates. We use a sliding window of consecutive frames to add temporal context. Our approach predicts the 13 Z-values (one per joint) for the last frame in each window and compares different neural network types, including dense ,cnn,gru, to see which one works best.\n",
    "\n",
    "1. Data Preparation:\n",
    "Load Kinect CSV files and extract the X and Y coordinates as inputs, with the Z coordinates as targets. Create training samples using a sliding window, making sure the window stays within one sequence (each CSV file represents one sequence).\n",
    "\n",
    "2. Model Building:\n",
    "Build TensorFlow/Keras models with hidden layers activated by ReLU and a linear output layer to predict raw Z-values. Use the Adam optimizer with Mean Squared Error as the loss function, and track Mean Absolute Error as an extra performance metric.\n",
    "\n",
    "3. Training & Evaluation:\n",
    "Use 10-fold cross-validation by splitting sequences into folds. Apply early stopping to avoid overfitting and use model checkpointing to save the best weights. Perform a grid search over key hyperparameters (window size, learning rate, architecture type, number of layers, and units per layer) and log the performance results to a CSV file for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddfd1ac",
   "metadata": {},
   "source": [
    "### Import Libraries and Configure GPU\n",
    "\n",
    "We import the necessary libraries for data handling (pandas, NumPy), data splitting and scaling (scikit-learn), and building neural networks (TensorFlow/Keras). We also enable GPU memory growth to avoid allocating all GPU memory at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Using CPU for training.\n",
      "TensorFlow version: 2.19.0\n",
      "GPUs: []\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Configure TensorFlow to use GPU efficiently (if available)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print(\"GPU found. Enabled memory growth on\", gpus[0].name)\n",
    "    except Exception as e:\n",
    "        print(\"Error enabling GPU memory growth:\", e)\n",
    "else:\n",
    "    print(\"No GPU found. Using CPU for training.\")\n",
    "    print(\"TensorFlow version:\", tf.__version__)\n",
    "    print(\"GPUs:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing\n",
    "\n",
    "Each Kinect CSV file holds one sequence of recorded joint coordinates. The code loads each file into a pandas DataFrame, drops the frame index column, and then separates the input features (columns ending in _x or _y) from the target outputs (columns ending in _z). The X inputs (26 features from 13 joints) and the Z targets (13 values) are converted to NumPy arrays and collected into lists. Later, we apply feature scaling to each training fold during cross-validation to standardize the inputs without risking data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing Kinect CSV files\n",
    "data_dir = \"data/kinect_sequences\"  # replace with your actual path\n",
    "csv_files = sorted([f for f in os.listdir(data_dir) if f.endswith(\".csv\")])\n",
    "\n",
    "# Lists to hold sequences of features (X) and targets (Z)\n",
    "sequences_X = []\n",
    "sequences_Z = []\n",
    "\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    try:\n",
    "        # Load the CSV into a pandas DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Successfully loaded {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Drop frame number column if present\n",
    "    if 'FrameNo' in df.columns or 'frame' in df.columns:\n",
    "        df = df.drop(columns=['FrameNo'], errors='ignore')\n",
    "        df = df.drop(columns=['frame'], errors='ignore')\n",
    "    \n",
    "    # Strip any whitespace from column names (if needed)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Separate feature and target columns\n",
    "    feature_cols = [col for col in df.columns if col.endswith('_x') or col.endswith('_y')]\n",
    "    target_cols = [col for col in df.columns if col.endswith('_z')]\n",
    "    \n",
    "    X_values = df[feature_cols].to_numpy()\n",
    "    Z_values = df[target_cols].to_numpy()\n",
    "    \n",
    "    sequences_X.append(X_values)\n",
    "    sequences_Z.append(Z_values)\n",
    "\n",
    "print(f\"Loaded {len(sequences_X)} sequences. Example sequence shape: {sequences_X[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window Data Generation\n",
    "\n",
    "We generate training samples by sliding a fixed-length window over each sequence, ensuring that windows do not cross sequence boundaries. Each sample uses the X and Y coordinates from W frames (shape WÃ—26) as input and the Z coordinates from the last frame (shape 13) as the target. The function below takes a sequence of inputs (X_seq) and targets (Z_seq) and returns all possible windowed samples as NumPy arrays. A quick test at the end shows how many samples are generated and their shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows_from_sequence(X_seq, Z_seq, window_size):\n",
    "    \"\"\"\n",
    "    Given a single sequence of features X_seq (shape: num_frames x 26) and \n",
    "    targets Z_seq (shape: num_frames x 13), generate all sliding window samples of length window_size.\n",
    "    Returns:\n",
    "      X_windows: array of shape (num_samples, window_size, 26)\n",
    "      Y_windows: array of shape (num_samples, 13) corresponding to Z of last frame in each window\n",
    "    \"\"\"\n",
    "    X_windows = []\n",
    "    Y_windows = []\n",
    "    num_frames = X_seq.shape[0]\n",
    "    if num_frames < window_size:\n",
    "        # Not enough frames for even one window\n",
    "        return np.array(X_windows), np.array(Y_windows)\n",
    "    for start in range(0, num_frames - window_size + 1):\n",
    "        end = start + window_size\n",
    "        # Stack frames [start, ..., end-1] as one window\n",
    "        X_w = X_seq[start:end]              # shape (window_size, 26)\n",
    "        Y_w = Z_seq[end-1]                 # shape (13,) - Z of last frame\n",
    "        X_windows.append(X_w)\n",
    "        Y_windows.append(Y_w)\n",
    "    # Convert to numpy arrays\n",
    "    X_windows = np.array(X_windows)\n",
    "    Y_windows = np.array(Y_windows)\n",
    "    return X_windows, Y_windows\n",
    "\n",
    "# Quick test on the first sequence (using a small window for demonstration)\n",
    "test_w = 5\n",
    "X_test_win, Y_test_win = create_windows_from_sequence(sequences_X[0], sequences_Z[0], window_size=test_w)\n",
    "print(f\"Created {X_test_win.shape[0]} window samples from one sequence (window={test_w}).\")\n",
    "print(\"Sample window input shape:\", X_test_win[0].shape, \"/ Sample window output shape:\", Y_test_win[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture Definition\n",
    "\n",
    "We define a function to build a Keras model based on a given architecture type and set of hyperparameters. This function lets us create models dynamically for our grid search. The parameters include:\n",
    "\n",
    "* arch: The architecture type ('dense', 'conv1d', 'lstm', 'hybrid', or 'cnn+lstm').\n",
    "\n",
    "* num_layers: Number of hidden layers (excluding the output layer). This value determines how many layers to add for each architecture:\n",
    "\n",
    "\n",
    "* units: Number of units/filters in each hidden layer. We use the same size for all layers.\n",
    "\n",
    "* learning_rate: The optimizerâ€™s learning rate.\n",
    "\n",
    "All models have an input shape of (window_size, 26) and output 13 values. Hidden layers use ReLU activation with He initialization, and the output layer uses linear activation for regression. The models are compiled with the Adam optimizer, Mean Squared Error loss, and we track Mean Absolute Error as a metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(window_size, arch, num_layers, units, learning_rate):\n",
    "    \"\"\"Constructs and compiles a Keras model given the architecture and hyperparameters.\"\"\"\n",
    "    model = models.Sequential()\n",
    "    # Define input shape (window_size timesteps, 26 features per timestep)\n",
    "    model.add(layers.Input(shape=(window_size, 26)))\n",
    "    \n",
    "    if arch == 'dense':\n",
    "        # Flatten time dimension and use Dense layers\n",
    "        model.add(layers.Flatten())  # shape becomes (window_size*26,)\n",
    "        for _ in range(num_layers):\n",
    "            model.add(layers.Dense(units, activation='relu', kernel_initializer='he_uniform'))\n",
    "        # Output layer\n",
    "        model.add(layers.Dense(13, activation='linear'))\n",
    "    \n",
    "    elif arch == 'conv1d':\n",
    "        # Conv1D layers across time dimension\n",
    "        for _ in range(num_layers):\n",
    "            model.add(layers.Conv1D(filters=units, kernel_size=3, padding='same',\n",
    "                                     activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(13, activation='linear'))\n",
    "    \n",
    "    elif arch == 'lstm':\n",
    "        # Stacked LSTM layers\n",
    "        for i in range(num_layers):\n",
    "            # If not the last LSTM layer, return sequences to feed next LSTM\n",
    "            return_seq = (i < num_layers - 1)\n",
    "            model.add(layers.LSTM(units, return_sequences=return_seq))\n",
    "        model.add(layers.Dense(13, activation='linear'))\n",
    "    \n",
    "    elif arch == 'hybrid':\n",
    "        # Combination of Conv1D and Dense layers\n",
    "        conv_count = num_layers // 2\n",
    "        dense_count = num_layers - conv_count\n",
    "        # Conv layers first\n",
    "        for _ in range(conv_count):\n",
    "            model.add(layers.Conv1D(filters=units, kernel_size=3, padding='same',\n",
    "                                     activation='relu', kernel_initializer='he_uniform'))\n",
    "        if conv_count > 0:\n",
    "            model.add(layers.Flatten())\n",
    "        # Dense layers after conv\n",
    "        for _ in range(dense_count):\n",
    "            model.add(layers.Dense(units, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(layers.Dense(13, activation='linear'))\n",
    "    \n",
    "    elif arch == 'cnn+lstm':\n",
    "        # Conv layers followed by a single LSTM layer\n",
    "        conv_layers = max(1, num_layers - 1)  # ensure at least 1 conv\n",
    "        for _ in range(conv_layers):\n",
    "            model.add(layers.Conv1D(filters=units, kernel_size=3, padding='same',\n",
    "                                     activation='relu', kernel_initializer='he_uniform'))\n",
    "        # Follow with one LSTM layer\n",
    "        model.add(layers.LSTM(units, return_sequences=False))\n",
    "        model.add(layers.Dense(13, activation='linear'))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown architecture type: {arch}\")\n",
    "    \n",
    "    # Compile the model with Adam optimizer and MSE loss\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Test the model creation function for one example configuration\n",
    "test_model = create_model(window_size=5, arch='conv1d', num_layers=3, units=64, learning_rate=0.001)\n",
    "print(test_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Cross-Validation and Hyperparameter Search\n",
    "\n",
    "With data loading, preprocessing, and model definition in place, we now set up a grid search over hyperparameters while performing 10-fold cross-validation for each combination. The hyperparameters we explore are:\n",
    "\n",
    "* Window sizes: [3, 5, 7, 9, 11, 13, 15, 17, 20]\n",
    "\n",
    "* Learning rates: [0.5, 0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
    "\n",
    "* Architectures: ['dense', 'conv1d', 'hybrid', 'lstm', 'cnn+lstm']\n",
    "\n",
    "* Number of layers: [2, 3, 4, 5, 6, 8, 10, 12] (this refers to the count of hidden layers, as defined per architecture)\n",
    "\n",
    "* Units per layer: [64, 128, 256, 512]\n",
    "\n",
    "We use a 10-fold cross-validation strategy where, for each fold:\n",
    "\n",
    "1. We split the sequences into training and testing sets using the KFold splitter (ensuring randomness by shuffling the sequence indices).\n",
    "\n",
    "2. We combine all frames from the training sequences to fit a StandardScaler, which is then used to transform both training and test features. This scaling is done after splitting to prevent data leakage.\n",
    "\n",
    "3. We generate sliding window samples on the scaled features for training and testing.\n",
    "\n",
    "4. We further split a portion (10%) of the training windows for validation to monitor early stopping.\n",
    "\n",
    "5. A new model is created using `create_model` with the current hyperparameters.\n",
    "\n",
    "6. The model is trained using EarlyStopping (monitoring 'val_loss' with a patience of 5 epochs and restoring the best weights) and ModelCheckpoint (saving the best weights to a file).\n",
    "\n",
    "7. After training, the model is evaluated on the test data, and the Mean Squared Error (MSE) and Mean Absolute Error (MAE) are recorded.\n",
    "\n",
    "8. We clear the model and session from memory before proceeding to the next fold.\n",
    "\n",
    "After all folds are complete for a given hyperparameter combination, we calculate the average MSE and MAE, record the training time, and log all the results (including each fold's MSE, the averages, and hyperparameter values) into a CSV file. This grid search is computationally heavy but is manageable with GPU acceleration and early stopping to limit unnecessary training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "window_sizes = [3, 5, 7, 9, 11, 13, 15, 17, 20]\n",
    "learning_rates = [0.5, 0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
    "architectures = ['dense', 'conv1d', 'hybrid', 'lstm', 'cnn+lstm']\n",
    "num_layers_list = [2, 3, 4, 5, 6, 8, 10, 12]\n",
    "units_list = [64, 128, 256, 512]\n",
    "epochs_per_fold = 50\n",
    "\n",
    "# Prepare K-fold splitter (we will shuffle the sequence indices for randomness)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# CSV file to log results\n",
    "results_file = \"experiment_results.csv\"\n",
    "# Write CSV header\n",
    "results_header = ([\"architecture\", \"window_size\", \"num_layers\", \"units\", \"learning_rate\"] +\n",
    "                  [f\"fold{i+1}_mse\" for i in range(10)] +\n",
    "                  [\"avg_mse\", \"avg_mae\", \"training_time_sec\"])\n",
    "with open(results_file, 'w') as f:\n",
    "    f.write(\",\".join(results_header) + \"\\n\")\n",
    "\n",
    "# Begin grid search\n",
    "import time\n",
    "experiment_count = 0\n",
    "total_experiments = (len(window_sizes) * len(learning_rates) * \n",
    "                     len(architectures) * len(num_layers_list) * len(units_list))\n",
    "print(f\"Total experiments to run: {total_experiments}\")\n",
    "for arch in architectures:\n",
    "    for window_size in window_sizes:\n",
    "        for num_layers in num_layers_list:\n",
    "            for units in units_list:\n",
    "                for lr in learning_rates:\n",
    "                    experiment_count += 1\n",
    "                    config_description = (f\"arch={arch}, window={window_size}, layers={num_layers}, \"\n",
    "                                           f\"units={units}, lr={lr}\")\n",
    "                    print(f\"\\n=== Experiment {experiment_count}/{total_experiments}: {config_description} ===\")\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    fold_mse_scores = []\n",
    "                    fold_mae_scores = []\n",
    "                    \n",
    "                    # Perform 10-fold cross-validation for this config\n",
    "                    fold_index = 1\n",
    "                    for train_idx, test_idx in kf.split(sequences_X):\n",
    "                        # Prepare training and testing data for this fold\n",
    "                        # Combine training sequences' frames to fit scaler\n",
    "                        train_frames = []\n",
    "                        for seq_idx in train_idx:\n",
    "                            train_frames.append(sequences_X[seq_idx])\n",
    "                        train_frames = np.vstack(train_frames)\n",
    "                        # Fit scaler on all training frames (for X features)\n",
    "                        scaler = StandardScaler().fit(train_frames)\n",
    "                        \n",
    "                        # Generate windowed data for training\n",
    "                        X_train_all = []\n",
    "                        Y_train_all = []\n",
    "                        for seq_idx in train_idx:\n",
    "                            # Scale the entire sequence's features\n",
    "                            X_seq = scaler.transform(sequences_X[seq_idx])\n",
    "                            Z_seq = sequences_Z[seq_idx]  # target can remain unscaled\n",
    "                            X_wins, Y_wins = create_windows_from_sequence(X_seq, Z_seq, window_size)\n",
    "                            if X_wins.size == 0:\n",
    "                                continue  # sequence too short for this window size\n",
    "                            X_train_all.append(X_wins)\n",
    "                            Y_train_all.append(Y_wins)\n",
    "                        if len(X_train_all) == 0:\n",
    "                            # If no training data (should not happen unless window_size > all seq lengths)\n",
    "                            continue\n",
    "                        # Concatenate all training windows from all sequences\n",
    "                        X_train_all = np.vstack(X_train_all)\n",
    "                        Y_train_all = np.vstack(Y_train_all)\n",
    "                        \n",
    "                        # Generate windowed data for testing (evaluation fold)\n",
    "                        X_test_all = []\n",
    "                        Y_test_all = []\n",
    "                        for seq_idx in test_idx:\n",
    "                            X_seq = scaler.transform(sequences_X[seq_idx])\n",
    "                            Z_seq = sequences_Z[seq_idx]\n",
    "                            X_wins, Y_wins = create_windows_from_sequence(X_seq, Z_seq, window_size)\n",
    "                            if X_wins.size == 0:\n",
    "                                continue  # If test sequence too short, it contributes no samples\n",
    "                            X_test_all.append(X_wins)\n",
    "                            Y_test_all.append(Y_wins)\n",
    "                        if len(X_test_all) == 0:\n",
    "                            # If no test data for this fold (all test sequences too short), skip fold\n",
    "                            print(f\"Fold {fold_index}: no test data (sequence too short for window={window_size}). Skipping.\")\n",
    "                            continue\n",
    "                        X_test_all = np.vstack(X_test_all)\n",
    "                        Y_test_all = np.vstack(Y_test_all)\n",
    "                        \n",
    "                        # Split off a validation set from training data for early stopping\n",
    "                        X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "                            X_train_all, Y_train_all, test_size=0.1, random_state=42)\n",
    "                        \n",
    "                        # Build model for this configuration\n",
    "                        model = create_model(window_size, arch, num_layers, units, lr)\n",
    "                        \n",
    "                        # Callbacks for early stopping and checkpointing\n",
    "                        callbacks = [\n",
    "                            EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "                            ModelCheckpoint(filepath='best_model.h5', monitor='val_loss',\n",
    "                                            save_best_only=True, save_weights_only=True)\n",
    "                        ]\n",
    "                        \n",
    "                        # Train the model\n",
    "                        model.fit(X_train, Y_train, epochs=epochs_per_fold, batch_size=32,\n",
    "                                  validation_data=(X_val, Y_val), callbacks=callbacks, verbose=0)\n",
    "                        \n",
    "                        # Load best weights (if not already restored by early stopping)\n",
    "                        # (EarlyStopping with restore_best_weights=True already did this, but we'll ensure by loading checkpoint)\n",
    "                        try:\n",
    "                            model.load_weights('best_model.h5')\n",
    "                        except Exception as e:\n",
    "                            pass  # If file not found (e.g., not saved because no improvement), ignore\n",
    "                        \n",
    "                        # Evaluate on the test set\n",
    "                        loss, mae = model.evaluate(X_test_all, Y_test_all, verbose=0)\n",
    "                        fold_mse_scores.append(loss)\n",
    "                        fold_mae_scores.append(mae)\n",
    "                        print(f\"Fold {fold_index} MSE: {loss:.6f}, MAE: {mae:.6f}\")\n",
    "                        \n",
    "                        # Clean up model to free memory before next fold\n",
    "                        tf.keras.backend.clear_session()\n",
    "                        del model\n",
    "                        fold_index += 1\n",
    "                    \n",
    "                    # Compute average metrics across folds\n",
    "                    avg_mse = float(np.mean(fold_mse_scores)) if fold_mse_scores else float('nan')\n",
    "                    avg_mae = float(np.mean(fold_mae_scores)) if fold_mae_scores else float('nan')\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(f\"Avg MSE: {avg_mse:.6f}, Avg MAE: {avg_mae:.6f}, Training time: {elapsed:.2f} sec\")\n",
    "                    \n",
    "                    # Log results to CSV\n",
    "                    result_data = [arch, window_size, num_layers, units, lr]\n",
    "                    # Add each fold's MSE\n",
    "                    for i in range(10):\n",
    "                        result_data.append(fold_mse_scores[i] if i < len(fold_mse_scores) else \"\")\n",
    "                    result_data += [avg_mse, avg_mae, elapsed]\n",
    "                    result_line = \",\".join(map(str, result_data))\n",
    "                    with open(results_file, 'a') as f:\n",
    "                        f.write(result_line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Experiment\n",
    "\n",
    "The grid search will run 787 configurations, each using 10-fold cross-validation. For every hyperparameter combination, training samples are generated, the model is trained with early stopping and checkpointing, and evaluation metrics (MSE and MAE) are recorded for each fold. All configuration details, fold metrics, average errors, and training times are logged incrementally to a CSV file, ensuring results are preserved even if the process is interrupted.\n",
    "best models result : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```markdown\n",
    "## âœ… Best Model Summary\n",
    "\n",
    "```\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #     \n",
    "=================================================================\n",
    " flatten_9 (Flatten)         (None, 260)               0           \n",
    " dense_63 (Dense)            (None, 128)               33408       \n",
    " dense_64 (Dense)            (None, 128)               16512       \n",
    " dense_65 (Dense)            (None, 128)               16512       \n",
    " dense_66 (Dense)            (None, 128)               16512       \n",
    " dense_67 (Dense)            (None, 128)               16512       \n",
    " dense_68 (Dense)            (None, 128)               16512       \n",
    " dense_69 (Dense)            (None, 13)                1677        \n",
    "=================================================================\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Š **Average Metrics Across 10-Fold Validation**\n",
    "- **RSS**: `96.631887`  \n",
    "- **MAE**: `0.037999`  \n",
    "- **MSE**: `0.003328`  \n",
    "- **Cross-Entropy**: `3.450090`  \n",
    "- **KL Divergence**: `2.313532`\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
